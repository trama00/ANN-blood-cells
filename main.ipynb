{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1731690348839,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "T-VoNk6p3r76"
   },
   "outputs": [],
   "source": [
    "USE_COLAB = True\n",
    "SHOW_MODEL_SUMMARY = True\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 1e-4\n",
    "AUGMENTATION = False\n",
    "BALANCE_SET = \"test\" # \"train\", \"val\", \"test\", \"train and val\", \"train and test\", \"val and test\" ( use test to not balance)\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.01\n",
    "USE_TEST = False\n",
    "USE_TEST_BIG = True\n",
    "MIXUP = True\n",
    "ALPHA_MIXUP = 0.2\n",
    "MIXUP_AUGMENT_FACTOR = 2.0\n",
    "CONV_LAYERS = 3\n",
    "DENSE_LAYERS = 3\n",
    "NODES_PER_LAYER = 512\n",
    "DROPOUT_RATE = 0.5\n",
    "PATIENCE = 20\n",
    "L2_REGULARIZATION = 1e-3\n",
    "USE_BASE_MODEL = True\n",
    "MODEL = 'ConvNeXtSmall'  # 'VGG19', 'ResNet50', 'ResNet50V2', 'ResNet101', 'ResNet101V2', 'ResNet152',\n",
    "# 'ResNet152V2', 'Xception', 'InceptionV3', 'InceptionResNetV2', 'MobileNet', 'MobileNetV2',\n",
    "# 'DenseNet121', 'DenseNet169', 'DenseNet201', 'NASNetMobile', 'NASNetLarge',\n",
    "# 'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4',\n",
    "# 'EfficientNetB5', 'EfficientNetB6', 'EfficientNetB7', 'EfficientNetV2B0', 'EfficientNetV2B1',\n",
    "# 'EfficientNetV2B2', 'EfficientNetV2B3', 'EfficientNetV2S', 'EfficientNetV2M', 'EfficientNetV2L',\n",
    "# 'ConvNeXtTiny', 'ConvNeXtSmall', 'ConvNeXtBase', 'ConvNeXtLarge', 'ConvNeXtXLarge'\n",
    "USE_BATCH_NORMALIZATION = False\n",
    "USE_CLASS_WEIGHTS = True\n",
    "BALANCE_TRAINING_CLASSES = False # Deprecated\n",
    "USE_PREPROCESSING = False\n",
    "BACKGROUND_THRESHOLD = 0.5 # if the background class has a probability higher than this threshold, the image is considered as background (set 1 if you want to disable this feature)\n",
    "SEED = 72121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1792,
     "status": "ok",
     "timestamp": 1731690350933,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "jOma3aMZ3r78",
    "outputId": "ee11d5f0-8498-4380-82c9-6271c9913bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "/gdrive/My Drive/ANN_net\n"
     ]
    }
   ],
   "source": [
    "if USE_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/gdrive')\n",
    "    %cd /gdrive/My Drive/ANN_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3800,
     "status": "ok",
     "timestamp": 1731690354731,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "t9WZMOrc3r78"
   },
   "outputs": [],
   "source": [
    "from libraries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2673,
     "status": "ok",
     "timestamp": 1731690357397,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "qkuXQWtp3r79"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('data/training_set.npz', allow_pickle=True)\n",
    "\n",
    "# Divide data\n",
    "labels = data['labels']\n",
    "images = data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13395,
     "status": "ok",
     "timestamp": 1731690370789,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "eZbqvUNR3r7_",
    "outputId": "8811d7f2-6a5a-4fa9-b192-e09b37c0ac7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1808 duplicate and unwanted images.\n"
     ]
    }
   ],
   "source": [
    "images, labels = clean_dataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1731690370790,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "KBXZkzP13r8A",
    "outputId": "8f7b19d1-3960-4da2-d4e8-f381c3da2d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 11951 images\n",
      "\n",
      "Images in dataset: 11951\n",
      "\n",
      "Dataset Split Sizes:\n",
      "------------------------------\n",
      "Train:       10158 images (85.00%)\n",
      "Validation:   1016 images (8.50%)\n",
      "Test:       Not created\n",
      "\n",
      "Train Set Distribution:\n",
      "Class          Count     Percentage\n",
      "-----------------------------------\n",
      "0                722          7.11%\n",
      "1               1852         18.23%\n",
      "2                922          9.08%\n",
      "3               1720         16.93%\n",
      "4                722          7.11%\n",
      "5                843          8.30%\n",
      "6               1980         19.49%\n",
      "7               1397         13.75%\n",
      "\n",
      "Validation Set Distribution:\n",
      "Class          Count     Percentage\n",
      "-----------------------------------\n",
      "0                127         12.50%\n",
      "1                127         12.50%\n",
      "2                127         12.50%\n",
      "3                127         12.50%\n",
      "4                127         12.50%\n",
      "5                127         12.50%\n",
      "6                127         12.50%\n",
      "7                127         12.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_and_balance_distribution(\n",
    "    images, labels, val_size=VAL_SPLIT, test_size=TEST_SPLIT, seed=SEED, balance_sets=BALANCE_SET, TEST=USE_TEST\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1731690370790,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "0n7EfQH93r8B"
   },
   "outputs": [],
   "source": [
    "if USE_TEST:\n",
    "    # One-hot encode labels\n",
    "    y_train, y_val, y_test = one_hot_encode_labels(y_train, y_val, y_test)\n",
    "else:\n",
    "    y_train, y_val = one_hot_encode_labels(y_train, y_val, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5238,
     "status": "ok",
     "timestamp": 1731690376017,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "ipzti3eS3r8B",
    "outputId": "b2628342-e516-41d5-a540-bcb6f242eacf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10158, 96, 96, 3)\n",
      "Generating 10158 additional samples using Mixup\n",
      "Generating 181 samples per class pair\n",
      "X_train shape after mixup: (20294, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "if MIXUP:\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    X_train, y_train = apply_mixup(X_train, y_train, alpha=ALPHA_MIXUP, factor=MIXUP_AUGMENT_FACTOR)\n",
    "    print(\"X_train shape after mixup:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731690376017,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "0g9AEOqV3r8C",
    "outputId": "dd2d1e5a-2b4c-4439-82e9-bd689c941b48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (96, 96, 3)\n",
      "Output shape: 8\n"
     ]
    }
   ],
   "source": [
    "# Preprocess function suited for ConvNeXt models\n",
    "X_train = preprocess_input(X_train)\n",
    "X_test = preprocess_input(X_test)\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "output_shape = y_train[0].shape[0]\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Output shape: {output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1731690376017,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "yYQkkqll3r8C"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    augmentation=None,\n",
    "    seed=SEED,\n",
    "    conv_layers=CONV_LAYERS,\n",
    "    dense_layers=DENSE_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    l2_regularization=L2_REGULARIZATION,\n",
    "    use_base_model=USE_BASE_MODEL,\n",
    "    background_threshold=BACKGROUND_THRESHOLD,\n",
    "    use_batch_normalization=USE_BATCH_NORMALIZATION,\n",
    "    nodes_per_layer=NODES_PER_LAYER,\n",
    "    use_preprocessing=USE_PREPROCESSING\n",
    "    ):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    relu_initialiser = tfk.initializers.HeNormal(seed=seed)\n",
    "    output_initialiser = tfk.initializers.GlorotNormal(seed=seed)\n",
    "    regularizer = tfk.regularizers.l2(l2_regularization)\n",
    "\n",
    "    # Define the input layer with original input shape\n",
    "    input_layer = tfk.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Preprocess the input image\n",
    "    if use_preprocessing:\n",
    "        x = PreprocessLayer(threshold=background_threshold)(input_layer)\n",
    "\n",
    "    else:\n",
    "        x = input_layer\n",
    "\n",
    "    if use_base_model:\n",
    "        # Load the VGG16 model with a custom input shape (96x96x3)\n",
    "        base_model = get_base_model(MODEL, input_shape=input_shape)\n",
    "\n",
    "        # Apply augmentation if specified\n",
    "        x = augmentation(x) if augmentation else x\n",
    "\n",
    "        x = base_model(x)\n",
    "\n",
    "        x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "        x = tfkl.Dropout(dropout_rate, name='dropout')(x)\n",
    "\n",
    "    else:\n",
    "        # Apply augmentation if specified\n",
    "        x = augmentation(x) if augmentation else x\n",
    "\n",
    "        # Add Conv layers\n",
    "        x = tfkl.Conv2D(filters=16, kernel_size=3, activation='relu',\n",
    "                       padding='same', name='first_conv')(x)\n",
    "        x = tfkl.MaxPooling2D((2, 2), name='first_maxpool')(x)\n",
    "\n",
    "        for i in range(conv_layers - 1):\n",
    "            num_filters = 32 * (2 ** i)\n",
    "            x = tfkl.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=3,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                name=f'conv_{num_filters}')(x)\n",
    "\n",
    "            if i < conv_layers - 2:  # Apply MaxPooling except for last conv layer\n",
    "                x = tfkl.MaxPooling2D((2, 2), name=f'maxpool_{num_filters}')(x)\n",
    "\n",
    "        # Apply GlobalAveragePooling2D after all conv layers\n",
    "        x = tfkl.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "\n",
    "        x = tfkl.Dropout(dropout_rate, name='dropout_0')(x)\n",
    "\n",
    "    # Add Dense layers\n",
    "    for i in range(dense_layers):\n",
    "        x = tfkl.Dense(int(nodes_per_layer/(2**i)),\n",
    "                      activation='relu',\n",
    "                      name=f'dense_{i+1}',\n",
    "                      kernel_initializer=relu_initialiser)(x)\n",
    "\n",
    "        if use_batch_normalization:\n",
    "            x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            x = tfkl.Dropout(dropout_rate, name=f'dropout_{i+1}')(x)\n",
    "\n",
    "    output_layer = tfkl.Dense(output_shape,\n",
    "                             activation='softmax',\n",
    "                             name='output_layer',\n",
    "                             kernel_initializer=output_initialiser,\n",
    "                             kernel_regularizer=regularizer\n",
    "                             if l2_regularization > 0 else None)(x)\n",
    "\n",
    "    # Create model\n",
    "    model = tfk.Model(input_layer, output_layer)\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tfk.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tfk.metrics.Precision(name='precision'),\n",
    "            tfk.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1731690376018,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "1--_r-Yw3r8D"
   },
   "outputs": [],
   "source": [
    "if AUGMENTATION:\n",
    "    augmentation_layers = tfk.Sequential([\n",
    "        tfkl.RandomFlip('horizontal'),\n",
    "        tfkl.RandomFlip('vertical'),\n",
    "        tfkl.RandomRotation(1),\n",
    "        tfkl.RandomTranslation(0.4, 0.4, fill_mode=''),\n",
    "        #tfkl.RandomCrop(64, 64),\n",
    "        #tfkl.RandomZoom(0.3, fill_mode='nearest'),\n",
    "        #tfkl.Resizing(96, 96)\n",
    "    ], name='augmentation')\n",
    "\n",
    "    augmentation = ConditionalAugmentation(augmentation_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "executionInfo": {
     "elapsed": 2353,
     "status": "ok",
     "timestamp": 1731690378363,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "juGlBDUZ3r8D",
    "outputId": "746fd2b0-131f-4d85-da59-1ad1a2e34a48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ convnext_small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">49,454,688</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ avg_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ convnext_small (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m768\u001b[0m)           │      \u001b[38;5;34m49,454,688\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ avg_pool (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m393,728\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,032\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,013,672</span> (190.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,013,672\u001b[0m (190.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,013,672</span> (190.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,013,672\u001b[0m (190.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(\n",
    "    augmentation=augmentation if AUGMENTATION else None\n",
    ")\n",
    "\n",
    "if SHOW_MODEL_SUMMARY:\n",
    "    model.summary()\n",
    "\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "checkpoint_callback = tfk.callbacks.ModelCheckpoint(\n",
    "    'models/best_model_restored.keras',  # Path where the model will be saved\n",
    "    monitor='val_accuracy',  # Metric to monitor\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    verbose=1,  # Print messages when saving the model\n",
    "    save_weights_only=False,  # Save the entire model (including architecture)\n",
    "    mode='max'  # 'max' to save the model with the highest validation accuracy\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, checkpoint_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731690378363,
     "user": {
      "displayName": "Rosario Napoli",
      "userId": "16200523913878401815"
     },
     "user_tz": -60
    },
    "id": "T0ZoMp8-ZFV6"
   },
   "outputs": [],
   "source": [
    "# class_weights = compute_class_weights(y_train) if USE_CLASS_WEIGHTS else None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "  class_weights = compute_class_weight(\n",
    "      class_weight='balanced',\n",
    "      classes=np.unique(np.argmax(y_train, axis=1)),\n",
    "      y=np.argmax(y_train, axis=1)\n",
    "  )\n",
    "\n",
    "  class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmM8aE0p3r8D",
    "outputId": "a6412f7e-1e3e-4fd5-9528-170435868946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693ms/step - accuracy: 0.1329 - loss: 4.3262 - precision: 0.1903 - recall: 0.0711\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50098, saving model to models/best_model_restored.keras\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - accuracy: 0.1331 - loss: 4.3113 - precision: 0.1905 - recall: 0.0708 - val_accuracy: 0.5010 - val_loss: 1.8993 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/500\n",
      "\u001b[1m 4/80\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 160ms/step - accuracy: 0.1911 - loss: 2.1315 - precision: 0.3509 - recall: 0.0170"
     ]
    }
   ],
   "source": [
    "# Train the model with early stopping callback\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights_dict\n",
    ").history\n",
    "\n",
    "final_val_acc = history['val_accuracy'][-(PATIENCE+1)] * 100\n",
    "print(f'Final validation accuracy: {final_val_acc:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN6L129O3r8E"
   },
   "outputs": [],
   "source": [
    "# Create a timestamp for the filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# save model using val acc\n",
    "model.save(f'models/model_{final_val_acc:.0f}_{timestamp}.keras')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "He3liZWo3r8E"
   },
   "outputs": [],
   "source": [
    "# Function to log model parameters to a text file\n",
    "def log_model_parameters(final_val_acc, timestamp):\n",
    "    # Create the log filename with date and time\n",
    "    log_filename = f'models/model_{final_val_acc:.0f}_params_{timestamp}.txt'\n",
    "\n",
    "    # Write the parameters to the log file\n",
    "    with open(log_filename, 'w') as log_file:\n",
    "        log_file.write(\"Model Training Parameters:\\n\\n\")\n",
    "        log_file.write(f\"BATCH_SIZE: {BATCH_SIZE}\\n\")\n",
    "        log_file.write(f\"EPOCHS: {EPOCHS}\\n\")\n",
    "        log_file.write(f\"LEARNING_RATE: {LEARNING_RATE}\\n\")\n",
    "        log_file.write(f\"AUGMENTATION: {AUGMENTATION}\\n\")\n",
    "        log_file.write(f\"MIXUP: {MIXUP}\\n\")\n",
    "        log_file.write(f\"CONV_LAYERS: {CONV_LAYERS}\\n\")\n",
    "        log_file.write(f\"DENSE_LAYERS: {DENSE_LAYERS}\\n\")\n",
    "        log_file.write(f\"NODES_PER_LAYER: {NODES_PER_LAYER}\\n\")\n",
    "        log_file.write(f\"DROPOUT_RATE: {DROPOUT_RATE}\\n\")\n",
    "        log_file.write(f\"PATIENCE: {PATIENCE}\\n\")\n",
    "        log_file.write(f\"L2_REGULARIZATION: {L2_REGULARIZATION}\\n\")\n",
    "        log_file.write(f\"USE_BASE_MODEL: {USE_BASE_MODEL}\\n\")\n",
    "        log_file.write(f\"USE_BATCH_NORMALIZATION: {USE_BATCH_NORMALIZATION}\\n\")\n",
    "        log_file.write(f\"USE_CLASS_WEIGHTS: {USE_CLASS_WEIGHTS}\\n\")\n",
    "        log_file.write(f\"BALANCE_TRAINING_CLASSES: {BALANCE_TRAINING_CLASSES}\\n\")\n",
    "        log_file.write(f\"SEED: {SEED}\\n\")\n",
    "\n",
    "\n",
    "# Log the model parameters\n",
    "log_model_parameters(final_val_acc, timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2mkb9rD3r8E"
   },
   "outputs": [],
   "source": [
    "# plot training loss and accuracy\n",
    "def plot_training(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axs[0].plot(history['loss'], label='train')\n",
    "    axs[0].plot(history['val_loss'], label='validation')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history['accuracy'], label='train')\n",
    "    axs[1].plot(history['val_accuracy'], label='validation')\n",
    "    axs[1].set_title('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zklVKxkk3r8E"
   },
   "source": [
    "# Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKzKsUVL3r8E"
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = tfk.models.load_model(f'models/model_{final_val_acc:.0f}_{timestamp}.keras', custom_objects={'PreprocessLayer': PreprocessLayer, 'ConditionalAugmentation': ConditionalAugmentation})\n",
    "#model = tfk.models.load_model(f'models/best_model_restored.keras', custom_objects={'PreprocessLayer': PreprocessLayer, 'ConditionalAugmentation': ConditionalAugmentation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S901W4VC3r8F"
   },
   "outputs": [],
   "source": [
    "# Main testing logic\n",
    "if USE_TEST_BIG:\n",
    "    test_data = np.load('data/blood_cells_96x96.npz', allow_pickle=True)\n",
    "    test_images = test_data['data']\n",
    "    test_labels = test_data['labels']\n",
    "\n",
    "\n",
    "    # Divide in 10 groups of tot images\n",
    "    N = 500\n",
    "    M = 10\n",
    "    images_per_class = N // 8\n",
    "\n",
    "    test_accuracy = 0.0\n",
    "    test_precision = 0.0\n",
    "    test_recall = 0.0\n",
    "    test_f1 = 0.0\n",
    "    for k in range(M):\n",
    "\n",
    "        print(f'Group {k+1}/{M}')\n",
    "\n",
    "        # choose images_per_class images for each class\n",
    "        group_labels = []\n",
    "        group_indices = []\n",
    "        group_images = []\n",
    "\n",
    "        for i in range(8):\n",
    "            indexes = np.where(test_labels == i)[0]\n",
    "            np.random.shuffle(indexes)\n",
    "            group_indices.extend(indexes[:images_per_class])\n",
    "\n",
    "        group_labels = test_labels[group_indices]\n",
    "        group_images = test_images[group_indices]\n",
    "\n",
    "        # Predict class probabilities and get predicted classes for normal test set\n",
    "        test_predictions = model.predict(group_images, verbose=1)\n",
    "        test_predictions_classes = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "        # Calculate and display metrics for the normal test set\n",
    "        test_accuracy += accuracy_score(group_labels, test_predictions_classes)\n",
    "        test_precision += precision_score(group_labels, test_predictions_classes, average='weighted')\n",
    "        test_recall += recall_score(group_labels, test_predictions_classes, average='weighted')\n",
    "        test_f1 += f1_score(group_labels, test_predictions_classes, average='weighted')\n",
    "\n",
    "    test_accuracy /= M\n",
    "    test_precision /= M\n",
    "    test_recall /= M\n",
    "    test_f1 /= M\n",
    "\n",
    "    print(f'Accuracy score over the normal test set: {round(test_accuracy, 4)}')\n",
    "    print(f'Precision score over the normal test set: {round(test_precision, 4)}')\n",
    "    print(f'Recall score over the normal test set: {round(test_recall, 4)}')\n",
    "    print(f'F1 score over the normal test set: {round(test_f1, 4)}')\n",
    "\n",
    "\n",
    "elif USE_TEST:\n",
    "    test_data = np.load('data/test_set.npz', allow_pickle=True)\n",
    "    test_images = test_data['data']\n",
    "    test_labels = test_data['labels']\n",
    "\n",
    "    # Predict class probabilities and get predicted classes for normal test set\n",
    "    test_predictions = model.predict(test_images, verbose=0)\n",
    "    test_predictions_classes = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "    # Extract ground truth classes\n",
    "    test_gt = np.argmax(test_labels, axis=-1)\n",
    "\n",
    "    # Calculate and display metrics for the normal test set\n",
    "    test_accuracy = accuracy_score(test_gt, test_predictions_classes)\n",
    "    test_precision = precision_score(test_gt, test_predictions_classes, average='weighted')\n",
    "    test_recall = recall_score(test_gt, test_predictions_classes, average='weighted')\n",
    "    test_f1 = f1_score(test_gt, test_predictions_classes, average='weighted')\n",
    "\n",
    "    print(f'Accuracy score over the normal test set: {round(test_accuracy, 4)}')\n",
    "    print(f'Precision score over the normal test set: {round(test_precision, 4)}')\n",
    "    print(f'Recall score over the normal test set: {round(test_recall, 4)}')\n",
    "    print(f'F1 score over the normal test set: {round(test_f1, 4)}')\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(test_gt, test_predictions_classes)\n",
    "\n",
    "    # Plot the confusion matrix with class labels\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d',\n",
    "                xticklabels=[f'Class {i}' for i in range(8)],\n",
    "                yticklabels=[f'Class {i}' for i in range(8)], cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report for detailed metrics per class\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(test_gt, test_predictions_classes))\n",
    "\n",
    "    # ROC-AUC score for each class (only if this is multilabel or multiclass with probability predictions)\n",
    "    y_test_binarized = label_binarize(test_gt, classes=range(8))\n",
    "    roc_auc_scores = []\n",
    "    for i in range(8):\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test_binarized[:, i], test_predictions[:, i])\n",
    "            roc_auc_scores.append(roc_auc)\n",
    "            print(f\"Class {i} ROC-AUC Score: {round(roc_auc, 4)}\")\n",
    "        except ValueError:\n",
    "            print(f\"Class {i} ROC-AUC Score: Unable to calculate (not enough samples).\")\n",
    "\n",
    "    # Optional: Display mean ROC-AUC score across classes\n",
    "    if roc_auc_scores:\n",
    "        print(f\"\\nMean ROC-AUC Score: {round(np.mean(roc_auc_scores), 4)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
