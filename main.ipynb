{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques to Improve Image Classification CNNs\n",
    "\n",
    "There are several techniques known to improve the accuracy, training efficiency, and generalization capability of convolutional neural networks (CNNs) for image classification. These techniques span across **data preprocessing and augmentation**, **model architecture**, **optimization and regularization**, **training strategy**, and **evaluation**. Below is a comprehensive list:\n",
    "\n",
    "## 1. Data Preprocessing and Augmentation\n",
    "\n",
    "- **Image Rescaling and Normalization**: Rescale pixel values to a range like \\([0, 1]\\) or standardize to zero mean and unit variance, which helps models converge faster.\n",
    "- **Data Augmentation**: Generate variations of images to improve generalization:\n",
    "  - **Random Cropping**\n",
    "  - **Random Flipping** (horizontal, vertical)\n",
    "  - **Color Jittering** (brightness, contrast, saturation)\n",
    "  - **Rotation** and **Scaling**\n",
    "  - **Random Zoom** and **Random Translation**\n",
    "  - **Gaussian Noise**: Adds noise to images to make the model robust to variations.\n",
    "  - **Mixup and CutMix**: Mixes two images or crops a portion of one image and places it in another, creating synthetic blended samples.\n",
    "- **Use of Generative Models**: Use GANs or VAEs to generate additional training samples for minority classes or to increase dataset diversity.\n",
    "\n",
    "## 2. Model Architecture Improvements\n",
    "\n",
    "- **Batch Normalization**: Helps stabilize and accelerate training by normalizing activations within each mini-batch.\n",
    "- **Residual Connections**: Use skip connections (as in ResNet) to help train deeper networks by alleviating the vanishing gradient problem.\n",
    "- **Attention Mechanisms**: Channel and spatial attention layers (e.g., SE blocks, CBAM) to allow the network to focus on important parts of the image.\n",
    "- **Depthwise Separable Convolutions**: Decompose convolutions into depthwise and pointwise convolutions to reduce model parameters while maintaining performance (used in MobileNet).\n",
    "- **Inception Modules**: Multi-scale processing by combining different kernel sizes within the same layer, as seen in GoogLeNet.\n",
    "- **Dilated/Atrous Convolutions**: Increase the receptive field without increasing the number of parameters, useful in segmentation and dense classification.\n",
    "- **Transformer Blocks**: Add attention-based transformer blocks for better global context, as in vision transformers (ViT).\n",
    "\n",
    "## 3. Optimization Techniques\n",
    "\n",
    "- **Adaptive Optimizers**: Use optimizers like **Adam**, **RMSprop**, or **AdamW** for faster convergence than traditional **SGD**.\n",
    "- **Learning Rate Scheduling**:\n",
    "  - **Step Decay**: Reduces the learning rate at fixed intervals.\n",
    "  - **Exponential Decay**: Reduces the learning rate continuously based on an exponential function.\n",
    "  - **Cosine Annealing**: Gradually reduces the learning rate following a cosine curve.\n",
    "  - **Warmup and Cosine Decay**: Starts with a lower learning rate and gradually warms up, then applies cosine decay.\n",
    "  - **Cyclical Learning Rates**: Varies the learning rate within a range, helping the model escape local minima.\n",
    "- **Gradient Clipping**: Limits the gradient magnitude to prevent exploding gradients in RNNs or very deep networks.\n",
    "\n",
    "## 4. Regularization Techniques\n",
    "\n",
    "- **Dropout**: Randomly turns off a fraction of neurons in each layer during training to prevent overfitting.\n",
    "- **L2 and L1 Regularization**: Adds penalties to the loss function to reduce model complexity and prevent overfitting.\n",
    "- **Label Smoothing**: Reduces confidence in the labels to make the model more robust.\n",
    "- **Early Stopping**: Stops training when the validation loss stops improving to prevent overfitting.\n",
    "- **Stochastic Weight Averaging (SWA)**: Averages weights over the last few epochs to improve generalization.\n",
    "\n",
    "## 5. Training Strategies\n",
    "\n",
    "- **Transfer Learning**: Use pre-trained models (e.g., ResNet, VGG) and fine-tune on a specific dataset to achieve higher accuracy with less data.\n",
    "- **Fine-Tuning**: Unfreeze top layers of a pre-trained model and retrain on the new dataset, adapting it to specific classes.\n",
    "- **Self-Supervised Learning**: Pretrain the model on unlabeled data, allowing it to learn useful features without labeled data.\n",
    "- **Curriculum Learning**: Start training on simpler tasks or easier samples, and gradually increase difficulty.\n",
    "- **Multi-Task Learning**: Train the model on related tasks simultaneously (e.g., classification and segmentation).\n",
    "- **Semi-Supervised Learning**: Use labeled and unlabeled data (e.g., with pseudo-labeling or consistency regularization).\n",
    "- **Active Learning**: Selectively label samples that the model is uncertain about to make training more data-efficient.\n",
    "\n",
    "## 6. Evaluation and Monitoring Techniques\n",
    "\n",
    "- **K-Fold Cross-Validation**: Use cross-validation to better estimate model performance and avoid overfitting.\n",
    "- **Ensembling Models**: Combine multiple models to reduce variance and improve accuracy (e.g., majority voting, averaging predictions).\n",
    "- **Model Checkpointing**: Save the best model during training to avoid loss of progress due to poor final epochs.\n",
    "- **Hyperparameter Tuning**: Use grid search or random search to find optimal hyperparameters.\n",
    "- **Monitoring with Validation Data**: Monitor validation loss and accuracy to detect overfitting or underfitting early on.\n",
    "\n",
    "## 7. Advanced Techniques\n",
    "\n",
    "- **Knowledge Distillation**: Train a smaller “student” model to mimic a larger “teacher” model’s outputs, achieving higher accuracy with a smaller model.\n",
    "- **Neural Architecture Search (NAS)**: Automates the search for optimal model architecture (e.g., EfficientNet).\n",
    "- **AutoAugment and RandAugment**: Automatically search for the best data augmentation policies.\n",
    "- **Learning Rate Finder**: Empirically find an optimal starting learning rate for faster convergence.\n",
    "\n",
    "Each of these techniques can be tailored to the dataset and model architecture to boost performance effectively. Using combinations of these methods, such as pre-trained models, data augmentation, dropout, learning rate scheduling, and hyperparameter tuning, is common in high-performing image classification models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations for Building an Image Classification Model\n",
    "\n",
    "Building a robust model for image classification involves various considerations and strategies. Here’s a comprehensive list of recommendations tailored for your dataset of 5,004 images:\n",
    "\n",
    "## 1. Data Preprocessing\n",
    "- **Normalization:** Scale pixel values to the range [0, 1] by dividing by 255.\n",
    "- **Resizing:** Ensure all images are resized to a consistent size (e.g., 96x96) to match your model's input requirements.\n",
    "- **Augmentation:** Implement data augmentation (random flips, rotations, shifts, brightness adjustments, etc.) to increase dataset variability and prevent overfitting.\n",
    "\n",
    "## 2. Dataset Split\n",
    "- **Training, Validation, Test Split:** Split your dataset into training (e.g., 70%), validation (e.g., 15%), and test (e.g., 15%) sets to evaluate model performance and generalization.\n",
    "- **Stratification:** If you have multiple classes, ensure that your split maintains the class distribution.\n",
    "\n",
    "## 3. Model Architecture\n",
    "- **Convolutional Layers:** Start with a stack of convolutional layers to extract features. Use ReLU activation for non-linearity.\n",
    "- **Pooling Layers:** Use MaxPooling layers to reduce spatial dimensions and retain important features.\n",
    "- **Dropout:** Incorporate Dropout layers to prevent overfitting, especially in dense layers.\n",
    "- **Dense Layers:** Add fully connected layers at the end, with the final layer having a softmax activation for multi-class classification.\n",
    "- **Batch Normalization:** Use Batch Normalization to stabilize training and improve convergence speed.\n",
    "\n",
    "## 4. Regularization Techniques\n",
    "- **Early Stopping:** Implement early stopping to halt training when validation performance starts to degrade.\n",
    "- **L2 Regularization:** Add L2 regularization in dense layers to reduce overfitting.\n",
    "- **Data Augmentation:** Continue augmenting data during training to expose the model to diverse examples.\n",
    "\n",
    "## 5. Optimization Strategy\n",
    "- **Learning Rate:** Use a learning rate scheduler to adjust the learning rate during training, or use adaptive optimizers like Adam or RMSprop.\n",
    "- **Loss Function:** Choose the appropriate loss function, such as categorical crossentropy for multi-class classification.\n",
    "- **Batch Size:** Experiment with different batch sizes (e.g., 32, 64) to find the best for your dataset.\n",
    "\n",
    "## 6. Evaluation Metrics\n",
    "- **Validation Metrics:** Track validation accuracy and loss to monitor model performance during training.\n",
    "- **Confusion Matrix:** Use a confusion matrix to evaluate class-specific performance and identify misclassifications.\n",
    "\n",
    "## 7. Model Experimentation\n",
    "- **Hyperparameter Tuning:** Experiment with various hyperparameters, including the number of layers, number of filters, kernel sizes, and dropout rates.\n",
    "- **Cross-Validation:** If feasible, consider k-fold cross-validation to ensure your model is robust and generalizes well across different splits of the data.\n",
    "\n",
    "## 8. Post-Training Evaluation\n",
    "- **Test Set Evaluation:** After training, evaluate the model on the test set to assess its performance on unseen data.\n",
    "- **Model Interpretation:** Consider using techniques such as Grad-CAM to visualize and interpret model predictions.\n",
    "\n",
    "## 9. Deployment Considerations\n",
    "- **Model Size:** Optimize your model for size if deploying on limited-resource environments (e.g., mobile devices).\n",
    "- **Inference Speed:** Ensure the model inference speed is acceptable for your application.\n",
    "\n",
    "## 10. Documentation and Versioning\n",
    "- **Document Experiments:** Keep track of different experiments, hyperparameters, and results for reproducibility and comparison.\n",
    "- **Model Versioning:** If using a framework like MLflow or DVC, consider versioning your models for better management.\n",
    "\n",
    "By following these recommendations, you can systematically build, evaluate, and refine your image classification model, potentially achieving better performance and generalization on your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_OUTLIERS = False\n",
    "SHOW_MODEL_SUMMARY = True\n",
    "BATCH_SIZE = 36\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 1e-4\n",
    "AUGMENTATION = True\n",
    "CONV_LAYERS = 3\n",
    "DENSE_LAYERS = 2\n",
    "NODES_PER_LAYER = 1024\n",
    "DROPOUT_RATE = 0.3\n",
    "PATIENCE = 100\n",
    "L2_REGULARIZATION = 0.1\n",
    "USE_BASE_MODEL = False\n",
    "USE_BATCH_NORMALIZATION = True\n",
    "USE_CLASS_WEIGHTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from keras import layers as tfkl\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SEED = 72121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('data/training_set.npz', allow_pickle=True)\n",
    "\n",
    "# Divide data\n",
    "labels = data['labels']\n",
    "images = data['images']\n",
    "\n",
    "# Print unique labels and their counts\n",
    "unique_labels = np.unique(labels)\n",
    "print(\"Unique labels in the dataset:\", unique_labels)\n",
    "for label in unique_labels:\n",
    "    count = np.sum(labels == label)\n",
    "    print(f\"Class {label}: {count} images\")\n",
    "\n",
    "# Create a dictionary to store indices for each class\n",
    "class_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
    "\n",
    "# Select one random image per class\n",
    "selected_indices = [np.random.choice(class_indices[label]) for label in unique_labels]\n",
    "\n",
    "# Plot one image per class\n",
    "num_classes = len(unique_labels)\n",
    "num_cols = 4  # You can adjust this for layout\n",
    "num_rows = (num_classes + num_cols - 1) // num_cols\n",
    "\n",
    "plt.figure(figsize=(15, 4 * num_rows))\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(np.clip(images[idx] / 255.0, 0, 1))\n",
    "    plt.title(f\"Class {labels[idx]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(images, labels, test_size=0.2, random_state=SEED, stratify=labels)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=SEED, stratify=y_train_val)\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Validation size: {X_val.shape[0]}\")\n",
    "print(f\"Test size: {X_test.shape[0]}\\n\")\n",
    "\n",
    "def print_class_distribution(y, set_name):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    percentages = counts / counts.sum() * 100\n",
    "    print(f\"{set_name} set:\")\n",
    "    for cls, percentage in zip(unique, percentages):\n",
    "        print(f\"Class {cls}: {percentage:.2f}%\")\n",
    "\n",
    "print_class_distribution(y_train, \"Train\")\n",
    "print_class_distribution(y_val, \"Validation\")\n",
    "print_class_distribution(y_test, \"Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of labeles\n",
    "y_train = tfk.utils.to_categorical(y_train)\n",
    "y_val = tfk.utils.to_categorical(y_val)\n",
    "y_test = tfk.utils.to_categorical(y_test)\n",
    "\n",
    "# print example\n",
    "print(\"Example of one hot encoded labels:\")\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape\n",
    "\n",
    "output_shape = y_train[0].shape[0]\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Output shape: {output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    augmentation=None,\n",
    "    seed=SEED,\n",
    "    conv_layers=CONV_LAYERS,\n",
    "    dense_layers=DENSE_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    l2_regularization=L2_REGULARIZATION,\n",
    "    use_base_model=USE_BASE_MODEL,\n",
    "    target_size=(224, 224)\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    relu_initialiser = tfk.initializers.HeNormal(seed=seed)\n",
    "    output_initialiser = tfk.initializers.GlorotNormal(seed=seed)\n",
    "    regularizer = tfk.regularizers.l2(l2_regularization)\n",
    "    \n",
    "    # Define the input layer with original input shape\n",
    "    input_layer = tfk.Input(shape=input_shape, name='input_layer')\n",
    "    \n",
    "    # Add preprocessing layers\n",
    "    if use_base_model:\n",
    "        x = tfkl.Resizing(*target_size, interpolation='bicubic')(input_layer)\n",
    "    x = tfkl.Rescaling(1./255)(x if use_base_model else input_layer)\n",
    "\n",
    "    \n",
    "    if use_base_model:\n",
    "        # Load the base model\n",
    "        base_model = ResNet50(include_top=False, \n",
    "                            input_shape=(target_size[0], target_size[1], 3), \n",
    "                            weights='imagenet')\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Apply augmentation if specified\n",
    "        x = augmentation(x) if augmentation else x\n",
    "        \n",
    "        x = base_model(x)    \n",
    "        x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        \n",
    "    else:\n",
    "        # Apply augmentation if specified\n",
    "        x = augmentation(x) if augmentation else x\n",
    "        \n",
    "        # Add Conv layers\n",
    "        x = tfkl.Conv2D(filters=16, kernel_size=3, activation='relu', \n",
    "                       padding='same', name='first_conv')(x)\n",
    "        x = tfkl.MaxPooling2D((2, 2), name='first_maxpool')(x)\n",
    "        \n",
    "        for i in range(conv_layers - 1):\n",
    "            num_filters = 32 * (2 ** i)\n",
    "            x = tfkl.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=3,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                name=f'conv_{num_filters}')(x)\n",
    "            \n",
    "            if i < conv_layers - 2:  # Apply MaxPooling except for last conv layer\n",
    "                x = tfkl.MaxPooling2D((2, 2), name=f'maxpool_{num_filters}')(x)\n",
    "        \n",
    "        # Apply GlobalAveragePooling2D after all conv layers\n",
    "        x = tfkl.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    \n",
    "    # Add Dense layers\n",
    "    for i in range(dense_layers):\n",
    "        x = tfkl.Dense(int(NODES_PER_LAYER/(2**i)),\n",
    "                      activation='relu',\n",
    "                      name=f'dense_{i+1}',\n",
    "                      kernel_initializer=relu_initialiser)(x)\n",
    "        \n",
    "        if USE_BATCH_NORMALIZATION:    \n",
    "            x = tfkl.BatchNormalization()(x)\n",
    "            \n",
    "        if dropout_rate > 0:    \n",
    "            x = tfkl.Dropout(dropout_rate, name=f'dropout_{i+1}')(x)\n",
    "    \n",
    "    output_layer = tfkl.Dense(output_shape, \n",
    "                             activation='softmax', \n",
    "                             name='output_layer', \n",
    "                             kernel_initializer=output_initialiser, \n",
    "                             kernel_regularizer=regularizer \n",
    "                             if l2_regularization > 0 else None)(x)\n",
    "\n",
    "    # Create model\n",
    "    model = tfk.Model(input_layer, output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tfk.optimizers.Adam(learning_rate=learning_rate),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUGMENTATION:\n",
    "    augmentation = tfk.Sequential([\n",
    "        tfkl.RandomFlip('horizontal'),\n",
    "        tfkl.RandomFlip('vertical'),\n",
    "        tfkl.RandomRotation(0.3),\n",
    "        tfkl.RandomZoom(0.2),\n",
    "        tfkl.RandomContrast(0.2),\n",
    "        tfkl.RandomTranslation(0.2, 0.2)\n",
    "        \n",
    "    ], name='augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    augmentation=augmentation if AUGMENTATION else None\n",
    ")\n",
    "\n",
    "if SHOW_MODEL_SUMMARY:\n",
    "    model.summary()\n",
    "\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    mode='auto',\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples\n",
    "num_samples = len(labels)\n",
    "\n",
    "# Initialize a dictionary to count occurrences of each class\n",
    "class_counts = {cls: 0 for cls in np.unique(labels)}\n",
    "\n",
    "# Count occurrences of each class\n",
    "for cls in np.unique(labels):\n",
    "    class_counts[cls] = len(labels[labels == cls])\n",
    "    print(f\"Class {cls}: {class_counts[cls]} samples\")\n",
    "\n",
    "# Calculate class weights\n",
    "class_weight = {i: 1 for i in range(8)}  # Default weights set to 1 for each class\n",
    "\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    class_weight = {\n",
    "        i: num_samples / (8 * class_counts[i]) for i in range(8)\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"Class weights: {class_weight}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with early stopping callback\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight\n",
    ").history\n",
    "\n",
    "final_val_acc = history['val_accuracy'][-(PATIENCE+1)]\n",
    "print(f'Final validation accuracy: {final_val_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamp for the filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# save model using val acc\n",
    "model.save(f'models/model_{final_val_acc:.2f}_{timestamp}.keras')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log model parameters to a text file\n",
    "def log_model_parameters(final_val_acc, timestamp):    \n",
    "    # Create the log filename with date and time\n",
    "    log_filename = f'models/model_{final_val_acc:.2f}_params_{timestamp}.txt'\n",
    "    \n",
    "    # Write the parameters to the log file\n",
    "    with open(log_filename, 'w') as log_file:\n",
    "        log_file.write(\"Model Training Parameters:\\n\\n\")\n",
    "        log_file.write(f\"BATCH_SIZE: {BATCH_SIZE}\\n\")\n",
    "        log_file.write(f\"EPOCHS: {EPOCHS}\\n\")\n",
    "        log_file.write(f\"LEARNING_RATE: {LEARNING_RATE}\\n\")\n",
    "        log_file.write(f\"AUGMENTATION: {AUGMENTATION}\\n\")\n",
    "        log_file.write(f\"CONV_LAYERS: {CONV_LAYERS}\\n\")\n",
    "        log_file.write(f\"DENSE_LAYERS: {DENSE_LAYERS}\\n\")\n",
    "        log_file.write(f\"NODES_PER_LAYER: {NODES_PER_LAYER}\\n\")\n",
    "        log_file.write(f\"DROPOUT_RATE: {DROPOUT_RATE}\\n\")\n",
    "        log_file.write(f\"PATIENCE: {PATIENCE}\\n\")\n",
    "        log_file.write(f\"L2_REGULARIZATION: {L2_REGULARIZATION}\\n\")\n",
    "        log_file.write(f\"USE_BASE_MODEL: {USE_BASE_MODEL}\\n\")\n",
    "        log_file.write(f\"USE_BATCH_NORMALIZATION: {USE_BATCH_NORMALIZATION}\\n\")\n",
    "        log_file.write(f\"USE_CLASS_WEIGHTS: {USE_CLASS_WEIGHTS}\\n\")\n",
    "        \n",
    "\n",
    "# Log the model parameters\n",
    "log_model_parameters(final_val_acc, timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss and accuracy\n",
    "def plot_training(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axs[0].plot(history['loss'], label='train')\n",
    "    axs[0].plot(history['val_loss'], label='validation')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history['accuracy'], label='train')\n",
    "    axs[1].plot(history['val_accuracy'], label='validation')\n",
    "    axs[1].set_title('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_training(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = tfk.models.load_model(f'models/model_{final_val_acc:.2f}_{timestamp}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities and get predicted classes\n",
    "test_predictions = model.predict(X_test, verbose=0)\n",
    "test_predictions = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "# Extract ground truth classes\n",
    "test_gt = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Calculate and display test set accuracy\n",
    "test_accuracy = accuracy_score(test_gt, test_predictions)\n",
    "print(f'Accuracy score over the test set: {round(test_accuracy, 4)}')\n",
    "\n",
    "# Calculate and display test set precision\n",
    "test_precision = precision_score(test_gt, test_predictions, average='weighted')\n",
    "print(f'Precision score over the test set: {round(test_precision, 4)}')\n",
    "\n",
    "# Calculate and display test set recall\n",
    "test_recall = recall_score(test_gt, test_predictions, average='weighted')\n",
    "print(f'Recall score over the test set: {round(test_recall, 4)}')\n",
    "\n",
    "# Calculate and display test set F1 score\n",
    "test_f1 = f1_score(test_gt, test_predictions, average='weighted')\n",
    "print(f'F1 score over the test set: {round(test_f1, 4)}')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(test_gt, test_predictions)\n",
    "\n",
    "# Create labels combining confusion matrix values\n",
    "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
    "\n",
    "# Plot the confusion matrix with class labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=labels, fmt='', \n",
    "            xticklabels=[f'Class {i}' for i in range(8)], \n",
    "            yticklabels=[f'Class {i}' for i in range(8)], cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
