{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1731606759469,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "T-VoNk6p3r76"
   },
   "outputs": [],
   "source": [
    "USE_COLAB = True\n",
    "SHOW_MODEL_SUMMARY = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 1e-4\n",
    "AUGMENTATION = False\n",
    "BALANCE_SET = \"train\" # \"train\", \"val\", \"test\", \"train and val\", \"train and test\", \"val and test\"\n",
    "MIXUP = True\n",
    "ALPHA_MIXUP = 0.2\n",
    "MIXUP_AUGMENT_FACTOR = 2.0\n",
    "CONV_LAYERS = 3\n",
    "DENSE_LAYERS = 3\n",
    "NODES_PER_LAYER = 512\n",
    "DROPOUT_RATE = 0.5\n",
    "PATIENCE = 40\n",
    "L2_REGULARIZATION = 1e-3\n",
    "USE_BASE_MODEL = True\n",
    "MODEL = 'ConvNeXtSmall'  # 'VGG19', 'ResNet50', 'ResNet50V2', 'ResNet101', 'ResNet101V2', 'ResNet152',\n",
    "# 'ResNet152V2', 'Xception', 'InceptionV3', 'InceptionResNetV2', 'MobileNet', 'MobileNetV2',\n",
    "# 'DenseNet121', 'DenseNet169', 'DenseNet201', 'NASNetMobile', 'NASNetLarge',\n",
    "# 'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4',\n",
    "# 'EfficientNetB5', 'EfficientNetB6', 'EfficientNetB7', 'EfficientNetV2B0', 'EfficientNetV2B1',\n",
    "# 'EfficientNetV2B2', 'EfficientNetV2B3', 'EfficientNetV2S', 'EfficientNetV2M', 'EfficientNetV2L',\n",
    "# 'ConvNeXtTiny', 'ConvNeXtSmall', 'ConvNeXtBase', 'ConvNeXtLarge', 'ConvNeXtXLarge'\n",
    "USE_BATCH_NORMALIZATION = False\n",
    "USE_CLASS_WEIGHTS = False\n",
    "BALANCE_TRAINING_CLASSES = False # Deprecated\n",
    "USE_PREPROCESSING = False\n",
    "BACKGROUND_THRESHOLD = 0.5 # if the background class has a probability higher than this threshold, the image is considered as background (set 1 if you want to disable this feature)\n",
    "SEED = 72121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19892,
     "status": "ok",
     "timestamp": 1731606779627,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "jOma3aMZ3r78",
    "outputId": "22ff0f8e-928a-4d7b-9589-38f489ad31b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive/My Drive/ANN/058_CNS_UNFREEZED\n"
     ]
    }
   ],
   "source": [
    "if USE_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/gdrive')\n",
    "    %cd /gdrive/My Drive/ANN/CHAL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10106,
     "status": "ok",
     "timestamp": 1731606789727,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "t9WZMOrc3r78"
   },
   "outputs": [],
   "source": [
    "from libraries import *\n",
    "from preprocess import balance_classes, one_hot_encode_labels, clean_dataset\n",
    "from data_partitioning import split_and_balance_distribution, print_class_distribution, apply_mixup\n",
    "from custom_layer import PreprocessLayer, ConditionalAugmentation\n",
    "from utils import get_base_model, analyze_mixup_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5049,
     "status": "ok",
     "timestamp": 1731606794770,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "qkuXQWtp3r79"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('data/training_set.npz', allow_pickle=True)\n",
    "\n",
    "# Divide data\n",
    "labels = data['labels']\n",
    "images = data['images']\n",
    "\n",
    "if (0):\n",
    "  # Print unique labels and their counts\n",
    "  unique_labels = np.unique(labels)\n",
    "  print(\"Unique labels in the dataset:\", unique_labels)\n",
    "  for label in unique_labels:\n",
    "      count = np.sum(labels == label)\n",
    "      print(f\"Class {label}: {count} images\")\n",
    "\n",
    "  # Create a dictionary to store the first index for each class\n",
    "  first_indices = {label: np.where(labels == label)[0][0] for label in unique_labels}\n",
    "\n",
    "  # Plot the first image per class\n",
    "  num_classes = len(unique_labels)\n",
    "  num_cols = 4  # You can adjust this for layout\n",
    "  num_rows = (num_classes + num_cols - 1) // num_cols\n",
    "\n",
    "  plt.figure(figsize=(15, 4 * num_rows))\n",
    "  for i, label in enumerate(unique_labels):\n",
    "      idx = first_indices[label]\n",
    "      plt.subplot(num_rows, num_cols, i + 1)\n",
    "      plt.imshow(np.clip(images[idx] / 255.0, 0, 1))\n",
    "      plt.title(f\"Class {labels[idx]}\")\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1731606794771,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "SJYZHkSA3r7-",
    "outputId": "d7acd580-4052-4353-8657-2265b8f7b07e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 - Avg Intensity: 0.8041, Darker Avg: 0.3032, Lighter Avg: 0.8206\n",
      "Image 2 - Avg Intensity: 0.6285, Darker Avg: 0.3020, Lighter Avg: 0.7325\n",
      "Image 3 - Avg Intensity: 0.7496, Darker Avg: 0.2688, Lighter Avg: 0.8029\n",
      "Image 4 - Avg Intensity: 0.6996, Darker Avg: 0.2885, Lighter Avg: 0.7695\n",
      "Image 5 - Avg Intensity: 0.7811, Darker Avg: 0.3534, Lighter Avg: 0.8025\n",
      "Image 6 - Avg Intensity: 0.7382, Darker Avg: 0.2656, Lighter Avg: 0.7991\n",
      "Image 7 - Avg Intensity: 0.7948, Darker Avg: 0.2735, Lighter Avg: 0.8306\n",
      "Image 8 - Avg Intensity: 0.7702, Darker Avg: 0.3202, Lighter Avg: 0.8219\n",
      "Image 9 - Avg Intensity: 0.7945, Darker Avg: 0.3568, Lighter Avg: 0.8018\n",
      "Image 10 - Avg Intensity: 0.7157, Darker Avg: 0.2702, Lighter Avg: 0.7531\n"
     ]
    }
   ],
   "source": [
    "# show that the background and the subject have different brightness\n",
    "sample_size = 10\n",
    "sample_images = images[:sample_size]  # Adjust as needed\n",
    "\n",
    "def analyze_brightness(image, threshold=BACKGROUND_THRESHOLD):\n",
    "    avg_intensity = np.mean(image)\n",
    "    darker_pixels = image[image < threshold]\n",
    "    lighter_pixels = image[image >= threshold]\n",
    "\n",
    "    avg_darker = np.mean(darker_pixels) if darker_pixels.size > 0 else 0.0\n",
    "    avg_lighter = np.mean(lighter_pixels) if lighter_pixels.size > 0 else 0.0\n",
    "\n",
    "    return avg_intensity, avg_darker, avg_lighter\n",
    "\n",
    "# Run analysis on the sample images\n",
    "for i, image in enumerate(sample_images):\n",
    "    avg_intensity, dark_avg, light_avg = analyze_brightness(image / 255.0)  # Normalize if needed\n",
    "    print(f\"Image {i + 1} - Avg Intensity: {avg_intensity:.4f}, Darker Avg: {dark_avg:.4f}, Lighter Avg: {light_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17975,
     "status": "ok",
     "timestamp": 1731606812740,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "eZbqvUNR3r7_",
    "outputId": "0078a656-f100-4fb3-d475-62ec30e389fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1808 duplicate and unwanted images.\n"
     ]
    }
   ],
   "source": [
    "images, labels = clean_dataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1731606813021,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "KBXZkzP13r8A",
    "outputId": "daecb0e6-10b9-4be1-a10b-ffefa37be90b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set Sizes:\n",
      "--------------------\n",
      "Train:        5704\n",
      "Validation:   1793\n",
      "Test:          120\n",
      "\n",
      "Train Set Distribution:\n",
      "Class          Count     Percentage\n",
      "-----------------------------------\n",
      "0                713         12.50%\n",
      "1                713         12.50%\n",
      "2                713         12.50%\n",
      "3                713         12.50%\n",
      "4                713         12.50%\n",
      "5                713         12.50%\n",
      "6                713         12.50%\n",
      "7                713         12.50%\n",
      "\n",
      "Validation Set Distribution:\n",
      "Class          Count     Percentage\n",
      "-----------------------------------\n",
      "0                127          7.08%\n",
      "1                327         18.24%\n",
      "2                163          9.09%\n",
      "3                303         16.90%\n",
      "4                127          7.08%\n",
      "5                149          8.31%\n",
      "6                350         19.52%\n",
      "7                247         13.78%\n",
      "\n",
      "Test Set Distribution:\n",
      "Class          Count     Percentage\n",
      "-----------------------------------\n",
      "0                  9          7.50%\n",
      "1                 22         18.33%\n",
      "2                 11          9.17%\n",
      "3                 20         16.67%\n",
      "4                  9          7.50%\n",
      "5                 10          8.33%\n",
      "6                 23         19.17%\n",
      "7                 16         13.33%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_and_balance_distribution(\n",
    "    images, labels, val_size=0.15, test_size=0.01, seed=SEED, balance_sets=BALANCE_SET\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731606813021,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "UHnXUwvA3r8A"
   },
   "outputs": [],
   "source": [
    "if BALANCE_TRAINING_CLASSES:\n",
    "    X_train, y_train = balance_classes(X_train, y_train, target_class_size=np.mean(np.bincount(labels)))\n",
    "    print_class_distribution(y_train, \"Train\")\n",
    "    print_class_distribution(y_val, \"Validation\")\n",
    "    print_class_distribution(y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731606813022,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "0n7EfQH93r8B"
   },
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train, y_val, y_test = one_hot_encode_labels(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3278,
     "status": "ok",
     "timestamp": 1731606816290,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "ipzti3eS3r8B",
    "outputId": "2861acc2-7107-4750-f709-8739f1d435b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5704, 96, 96, 3)\n",
      "Generating 5704 additional samples using Mixup\n",
      "Generating 101 samples per class pair\n",
      "X_train shape after mixup: (11360, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "X_train, y_train = apply_mixup(X_train, y_train, alpha=ALPHA_MIXUP, factor=MIXUP_AUGMENT_FACTOR)\n",
    "print(\"X_train shape after mixup:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1731606816290,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "0g9AEOqV3r8C",
    "outputId": "f581f908-bb9f-4c49-db41-4e6b7ae1c8bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (96, 96, 3)\n",
      "Output shape: 8\n"
     ]
    }
   ],
   "source": [
    "X_train = preprocess_input(X_train)\n",
    "X_test = preprocess_input(X_test)\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "\n",
    "output_shape = y_train[0].shape[0]\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Output shape: {output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731606816290,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "yYQkkqll3r8C"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    augmentation=None,\n",
    "    seed=SEED,\n",
    "    conv_layers=CONV_LAYERS,\n",
    "    dense_layers=DENSE_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    l2_regularization=L2_REGULARIZATION,\n",
    "    use_base_model=USE_BASE_MODEL,\n",
    "    background_threshold=BACKGROUND_THRESHOLD,\n",
    "    use_batch_normalization=USE_BATCH_NORMALIZATION,\n",
    "    nodes_per_layer=NODES_PER_LAYER,\n",
    "    use_preprocessing=USE_PREPROCESSING\n",
    "    ):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    relu_initialiser = tfk.initializers.HeNormal(seed=seed)\n",
    "    output_initialiser = tfk.initializers.GlorotNormal(seed=seed)\n",
    "    regularizer = tfk.regularizers.l2(l2_regularization)\n",
    "\n",
    "    # Define the input layer with original input shape\n",
    "    input_layer = tfk.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Preprocess the input image\n",
    "    if use_preprocessing:\n",
    "        x = PreprocessLayer(threshold=background_threshold)(input_layer)\n",
    "\n",
    "    else:\n",
    "        x = input_layer\n",
    "\n",
    "    if use_base_model:\n",
    "        # Load the VGG16 model with a custom input shape (96x96x3)\n",
    "        base_model = get_base_model(MODEL, input_shape=input_shape)\n",
    "\n",
    "        # Apply augmentation if specified\n",
    "        x = augmentation(x) if augmentation else x\n",
    "\n",
    "        x = base_model(x)\n",
    "        x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "    else:\n",
    "        # Apply augmentation if specified\n",
    "        x = augmentation(x) if augmentation else x\n",
    "\n",
    "        # Add Conv layers\n",
    "        x = tfkl.Conv2D(filters=16, kernel_size=3, activation='relu',\n",
    "                       padding='same', name='first_conv')(x)\n",
    "        x = tfkl.MaxPooling2D((2, 2), name='first_maxpool')(x)\n",
    "\n",
    "        for i in range(conv_layers - 1):\n",
    "            num_filters = 32 * (2 ** i)\n",
    "            x = tfkl.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=3,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                name=f'conv_{num_filters}')(x)\n",
    "\n",
    "            if i < conv_layers - 2:  # Apply MaxPooling except for last conv layer\n",
    "                x = tfkl.MaxPooling2D((2, 2), name=f'maxpool_{num_filters}')(x)\n",
    "\n",
    "        # Apply GlobalAveragePooling2D after all conv layers\n",
    "        x = tfkl.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "\n",
    "    # Add Dense layers\n",
    "    for i in range(dense_layers):\n",
    "        x = tfkl.Dense(int(nodes_per_layer/(2**i)),\n",
    "                      activation='relu',\n",
    "                      name=f'dense_{i+1}',\n",
    "                      kernel_initializer=relu_initialiser)(x)\n",
    "\n",
    "        if use_batch_normalization:\n",
    "            x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            x = tfkl.Dropout(dropout_rate, name=f'dropout_{i+1}')(x)\n",
    "\n",
    "    output_layer = tfkl.Dense(output_shape,\n",
    "                             activation='softmax',\n",
    "                             name='output_layer',\n",
    "                             kernel_initializer=output_initialiser,\n",
    "                             kernel_regularizer=regularizer\n",
    "                             if l2_regularization > 0 else None)(x)\n",
    "\n",
    "    # Create model\n",
    "    model = tfk.Model(input_layer, output_layer)\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tfk.optimizers.Adam(learning_rate=learning_rate),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731606816291,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "1--_r-Yw3r8D"
   },
   "outputs": [],
   "source": [
    "if AUGMENTATION:\n",
    "    augmentation_layers = tfk.Sequential([\n",
    "        #tfkl.RandomFlip('horizontal'),\n",
    "        #tfkl.RandomFlip('vertical'),\n",
    "        #tfkl.RandomRotation(0.3),\n",
    "        #tfkl.RandomTranslation(0.4, 0.4, fill_mode='nearest'),\n",
    "        tfkl.RandomCrop(64, 64),\n",
    "        tfkl.RandomZoom(0.3, fill_mode='nearest'),\n",
    "        tfkl.Resizing(96, 96)\n",
    "    ], name='augmentation')\n",
    "\n",
    "    augmentation = ConditionalAugmentation(augmentation_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4679,
     "status": "ok",
     "timestamp": 1731606820966,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "juGlBDUZ3r8D",
    "outputId": "b8f5faf9-ba21-4d24-9b15-5f530fcb6793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_small_notop.h5\n",
      "\u001b[1m198551472/198551472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    augmentation=augmentation if AUGMENTATION else None\n",
    ")\n",
    "\n",
    "if SHOW_MODEL_SUMMARY:\n",
    "    model.summary()\n",
    "\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "checkpoint_callback = tfk.callbacks.ModelCheckpoint(\n",
    "    'models/best_model_restored.keras',  # Path where the model will be saved\n",
    "    monitor='val_accuracy',  # Metric to monitor\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    verbose=1,  # Print messages when saving the model\n",
    "    save_weights_only=False,  # Save the entire model (including architecture)\n",
    "    mode='max'  # 'max' to save the model with the highest validation accuracy\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, checkpoint_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1731606820967,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "_ZBAzjiL3r8D",
    "outputId": "9db0bb86-5373-4169-aafd-c837445bbc21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 850 samples\n",
      "Class 1: 2179 samples\n",
      "Class 2: 1085 samples\n",
      "Class 3: 2023 samples\n",
      "Class 4: 849 samples\n",
      "Class 5: 992 samples\n",
      "Class 6: 2330 samples\n",
      "Class 7: 1643 samples\n",
      "Class weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of samples\n",
    "num_samples = len(labels)\n",
    "\n",
    "# Convert labels to numpy array if not already\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Get unique classes\n",
    "unique_classes = np.unique(labels)\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "# Initialize a dictionary to count occurrences of each class\n",
    "class_counts = {cls: 0 for cls in unique_classes}\n",
    "\n",
    "# Count occurrences of each class\n",
    "for cls in unique_classes:\n",
    "    class_counts[cls] = np.sum(labels == cls)\n",
    "    print(f\"Class {cls}: {class_counts[cls]} samples\")\n",
    "\n",
    "# Calculate class weights\n",
    "class_weight = {i: 1.0 for i in unique_classes}  # Default weights set to 1 for each class\n",
    "\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    # Calculate balanced weights\n",
    "    class_weight = {\n",
    "        i: num_samples / (num_classes * class_counts[i])\n",
    "        for i in unique_classes\n",
    "    }\n",
    "\n",
    "print(f\"Class weights: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1859622,
     "status": "error",
     "timestamp": 1731608680585,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "rmM8aE0p3r8D",
    "outputId": "492123b0-b9c8-4bc3-ce81-dbf8e96c21b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.2732 - loss: 2.6013\n",
      "Epoch 1: val_accuracy improved from -inf to 0.89069, saving model to models/best_model_restored.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 279ms/step - accuracy: 0.2737 - loss: 2.5990 - val_accuracy: 0.8907 - val_loss: 0.3570\n",
      "Epoch 2/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.7908 - loss: 0.9143\n",
      "Epoch 2: val_accuracy improved from 0.89069 to 0.96877, saving model to models/best_model_restored.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 242ms/step - accuracy: 0.7909 - loss: 0.9141 - val_accuracy: 0.9688 - val_loss: 0.1566\n",
      "Epoch 3/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8992 - loss: 0.6680\n",
      "Epoch 3: val_accuracy improved from 0.96877 to 0.98048, saving model to models/best_model_restored.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 254ms/step - accuracy: 0.8993 - loss: 0.6679 - val_accuracy: 0.9805 - val_loss: 0.0849\n",
      "Epoch 4/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9424 - loss: 0.5655\n",
      "Epoch 4: val_accuracy did not improve from 0.98048\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 157ms/step - accuracy: 0.9425 - loss: 0.5654 - val_accuracy: 0.9654 - val_loss: 0.1621\n",
      "Epoch 5/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9516 - loss: 0.5410\n",
      "Epoch 5: val_accuracy did not improve from 0.98048\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 158ms/step - accuracy: 0.9516 - loss: 0.5409 - val_accuracy: 0.9805 - val_loss: 0.0936\n",
      "Epoch 6/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9528 - loss: 0.5445\n",
      "Epoch 6: val_accuracy did not improve from 0.98048\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 156ms/step - accuracy: 0.9528 - loss: 0.5444 - val_accuracy: 0.9782 - val_loss: 0.1054\n",
      "Epoch 7/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9619 - loss: 0.4901\n",
      "Epoch 7: val_accuracy did not improve from 0.98048\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 159ms/step - accuracy: 0.9619 - loss: 0.4901 - val_accuracy: 0.9799 - val_loss: 0.0917\n",
      "Epoch 8/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9617 - loss: 0.4781\n",
      "Epoch 8: val_accuracy did not improve from 0.98048\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 158ms/step - accuracy: 0.9617 - loss: 0.4780 - val_accuracy: 0.9755 - val_loss: 0.1189\n",
      "Epoch 9/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9548 - loss: 0.4980\n",
      "Epoch 9: val_accuracy improved from 0.98048 to 0.98160, saving model to models/best_model_restored.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 244ms/step - accuracy: 0.9548 - loss: 0.4979 - val_accuracy: 0.9816 - val_loss: 0.0994\n",
      "Epoch 10/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9548 - loss: 0.4707\n",
      "Epoch 10: val_accuracy improved from 0.98160 to 0.98773, saving model to models/best_model_restored.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 265ms/step - accuracy: 0.9549 - loss: 0.4706 - val_accuracy: 0.9877 - val_loss: 0.0774\n",
      "Epoch 11/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9595 - loss: 0.4368\n",
      "Epoch 11: val_accuracy did not improve from 0.98773\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 157ms/step - accuracy: 0.9595 - loss: 0.4368 - val_accuracy: 0.9782 - val_loss: 0.1172\n",
      "Epoch 12/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9580 - loss: 0.4546\n",
      "Epoch 12: val_accuracy improved from 0.98773 to 0.99052, saving model to models/best_model_restored.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 258ms/step - accuracy: 0.9580 - loss: 0.4545 - val_accuracy: 0.9905 - val_loss: 0.0611\n",
      "Epoch 13/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9552 - loss: 0.4208\n",
      "Epoch 13: val_accuracy did not improve from 0.99052\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 157ms/step - accuracy: 0.9552 - loss: 0.4207 - val_accuracy: 0.9894 - val_loss: 0.0890\n",
      "Epoch 14/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9546 - loss: 0.4144\n",
      "Epoch 14: val_accuracy did not improve from 0.99052\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 158ms/step - accuracy: 0.9546 - loss: 0.4144 - val_accuracy: 0.9855 - val_loss: 0.0978\n",
      "Epoch 15/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9545 - loss: 0.4011\n",
      "Epoch 15: val_accuracy did not improve from 0.99052\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 157ms/step - accuracy: 0.9545 - loss: 0.4011 - val_accuracy: 0.9816 - val_loss: 0.1542\n",
      "Epoch 16/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9496 - loss: 0.4020\n",
      "Epoch 16: val_accuracy did not improve from 0.99052\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 158ms/step - accuracy: 0.9496 - loss: 0.4020 - val_accuracy: 0.9788 - val_loss: 0.0971\n",
      "Epoch 17/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9513 - loss: 0.3955\n",
      "Epoch 17: val_accuracy did not improve from 0.99052\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 157ms/step - accuracy: 0.9513 - loss: 0.3954 - val_accuracy: 0.9805 - val_loss: 0.1446\n",
      "Epoch 18/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9446 - loss: 0.3952\n",
      "Epoch 18: val_accuracy did not improve from 0.99052\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 157ms/step - accuracy: 0.9446 - loss: 0.3952 - val_accuracy: 0.9632 - val_loss: 0.2941\n",
      "Epoch 19/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9422 - loss: 0.3890\n",
      "Epoch 19: val_accuracy did not improve from 0.99052\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 158ms/step - accuracy: 0.9422 - loss: 0.3890 - val_accuracy: 0.9877 - val_loss: 0.0974\n",
      "Epoch 20/500\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9465 - loss: 0.3593\n",
      "Epoch 20: val_accuracy did not improve from 0.99052\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 158ms/step - accuracy: 0.9465 - loss: 0.3593 - val_accuracy: 0.9833 - val_loss: 0.1082\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-819b46ad75d0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model with early stopping callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model with early stopping callback\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight\n",
    ").history\n",
    "\n",
    "final_val_acc = history['val_accuracy'][-(PATIENCE+1)] * 100\n",
    "print(f'Final validation accuracy: {final_val_acc:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1731608680587,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "qN6L129O3r8E"
   },
   "outputs": [],
   "source": [
    "# Create a timestamp for the filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# save model using val acc\n",
    "model.save(f'models/model_{final_val_acc:.0f}_{timestamp}.keras')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1731608680588,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "He3liZWo3r8E"
   },
   "outputs": [],
   "source": [
    "# Function to log model parameters to a text file\n",
    "def log_model_parameters(final_val_acc, timestamp):\n",
    "    # Create the log filename with date and time\n",
    "    log_filename = f'models/model_{final_val_acc:.0f}_params_{timestamp}.txt'\n",
    "\n",
    "    # Write the parameters to the log file\n",
    "    with open(log_filename, 'w') as log_file:\n",
    "        log_file.write(\"Model Training Parameters:\\n\\n\")\n",
    "        log_file.write(f\"BATCH_SIZE: {BATCH_SIZE}\\n\")\n",
    "        log_file.write(f\"EPOCHS: {EPOCHS}\\n\")\n",
    "        log_file.write(f\"LEARNING_RATE: {LEARNING_RATE}\\n\")\n",
    "        log_file.write(f\"AUGMENTATION: {AUGMENTATION}\\n\")\n",
    "        log_file.write(f\"MIXUP: {MIXUP}\\n\")\n",
    "        log_file.write(f\"CONV_LAYERS: {CONV_LAYERS}\\n\")\n",
    "        log_file.write(f\"DENSE_LAYERS: {DENSE_LAYERS}\\n\")\n",
    "        log_file.write(f\"NODES_PER_LAYER: {NODES_PER_LAYER}\\n\")\n",
    "        log_file.write(f\"DROPOUT_RATE: {DROPOUT_RATE}\\n\")\n",
    "        log_file.write(f\"PATIENCE: {PATIENCE}\\n\")\n",
    "        log_file.write(f\"L2_REGULARIZATION: {L2_REGULARIZATION}\\n\")\n",
    "        log_file.write(f\"USE_BASE_MODEL: {USE_BASE_MODEL}\\n\")\n",
    "        log_file.write(f\"USE_BATCH_NORMALIZATION: {USE_BATCH_NORMALIZATION}\\n\")\n",
    "        log_file.write(f\"USE_CLASS_WEIGHTS: {USE_CLASS_WEIGHTS}\\n\")\n",
    "        log_file.write(f\"BALANCE_TRAINING_CLASSES: {BALANCE_TRAINING_CLASSES}\\n\")\n",
    "        log_file.write(f\"SEED: {SEED}\\n\")\n",
    "\n",
    "\n",
    "# Log the model parameters\n",
    "log_model_parameters(final_val_acc, timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1731608680588,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "V2mkb9rD3r8E"
   },
   "outputs": [],
   "source": [
    "# plot training loss and accuracy\n",
    "def plot_training(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axs[0].plot(history['loss'], label='train')\n",
    "    axs[0].plot(history['val_loss'], label='validation')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history['accuracy'], label='train')\n",
    "    axs[1].plot(history['val_accuracy'], label='validation')\n",
    "    axs[1].set_title('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zklVKxkk3r8E"
   },
   "source": [
    "# Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1731608680588,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "VKzKsUVL3r8E"
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = tfk.models.load_model(f'models/model_{final_val_acc:.0f}_{timestamp}.keras', custom_objects={'PreprocessLayer': PreprocessLayer, 'ConditionalAugmentation': ConditionalAugmentation})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1731608680589,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "S901W4VC3r8F"
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities and get predicted classes\n",
    "test_predictions = model.predict(X_test, verbose=0)\n",
    "test_predictions_classes = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "# Extract ground truth classes\n",
    "test_gt = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Calculate and display test set accuracy\n",
    "test_accuracy = accuracy_score(test_gt, test_predictions_classes)\n",
    "print(f'Accuracy score over the test set: {round(test_accuracy, 4)}')\n",
    "\n",
    "# Calculate and display test set precision\n",
    "test_precision = precision_score(test_gt, test_predictions_classes, average='weighted')\n",
    "print(f'Precision score over the test set: {round(test_precision, 4)}')\n",
    "\n",
    "# Calculate and display test set recall\n",
    "test_recall = recall_score(test_gt, test_predictions_classes, average='weighted')\n",
    "print(f'Recall score over the test set: {round(test_recall, 4)}')\n",
    "\n",
    "# Calculate and display test set F1 score\n",
    "test_f1 = f1_score(test_gt, test_predictions_classes, average='weighted')\n",
    "print(f'F1 score over the test set: {round(test_f1, 4)}')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(test_gt, test_predictions_classes)\n",
    "\n",
    "# Plot the confusion matrix with class labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d',\n",
    "            xticklabels=[f'Class {i}' for i in range(8)],\n",
    "            yticklabels=[f'Class {i}' for i in range(8)], cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n",
    "\n",
    "# Classification report for detailed metrics per class\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(test_gt, test_predictions_classes))\n",
    "\n",
    "# ROC-AUC score for each class (only if this is multilabel or multiclass with probability predictions)\n",
    "# Binarize the output classes for AUC calculation\n",
    "y_test_binarized = label_binarize(test_gt, classes=range(8))\n",
    "roc_auc_scores = []\n",
    "for i in range(8):\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test_binarized[:, i], test_predictions[:, i])\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        print(f\"Class {i} ROC-AUC Score: {round(roc_auc, 4)}\")\n",
    "    except ValueError:\n",
    "        print(f\"Class {i} ROC-AUC Score: Unable to calculate (not enough samples).\")\n",
    "\n",
    "# Optional: Display mean ROC-AUC score across classes\n",
    "if roc_auc_scores:\n",
    "    print(f\"\\nMean ROC-AUC Score: {round(np.mean(roc_auc_scores), 4)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "annEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
