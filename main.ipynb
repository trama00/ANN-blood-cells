{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1731606759469,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "T-VoNk6p3r76"
   },
   "outputs": [],
   "source": [
    "USE_COLAB = False\n",
    "SHOW_MODEL_SUMMARY = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "AUGMENTATION = False\n",
    "BALANCE_SET = \"TEST\" # \"train\", \"val\", \"test\", \"train and val\", \"train and test\", \"val and test\" ( use test to not balance)\n",
    "VAL_SPLIT = 0.25\n",
    "TEST_SPLIT = 0.000001\n",
    "USE_TEST = False\n",
    "USE_TEST_BIG = True\n",
    "MIXUP = True\n",
    "ALPHA_MIXUP = 0.2\n",
    "MIXUP_AUGMENT_FACTOR = 2.0\n",
    "CONV_LAYERS = 3\n",
    "DENSE_LAYERS = 3\n",
    "NODES_PER_LAYER = 512\n",
    "DROPOUT_RATE = 0.5\n",
    "PATIENCE = 2\n",
    "L2_REGULARIZATION = 1e-3\n",
    "USE_BASE_MODEL = False\n",
    "MODEL = 'ConvNeXtSmall'  # 'VGG19', 'ResNet50', 'ResNet50V2', 'ResNet101', 'ResNet101V2', 'ResNet152',\n",
    "# 'ResNet152V2', 'Xception', 'InceptionV3', 'InceptionResNetV2', 'MobileNet', 'MobileNetV2',\n",
    "# 'DenseNet121', 'DenseNet169', 'DenseNet201', 'NASNetMobile', 'NASNetLarge',\n",
    "# 'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4',\n",
    "# 'EfficientNetB5', 'EfficientNetB6', 'EfficientNetB7', 'EfficientNetV2B0', 'EfficientNetV2B1',\n",
    "# 'EfficientNetV2B2', 'EfficientNetV2B3', 'EfficientNetV2S', 'EfficientNetV2M', 'EfficientNetV2L',\n",
    "# 'ConvNeXtTiny', 'ConvNeXtSmall', 'ConvNeXtBase', 'ConvNeXtLarge', 'ConvNeXtXLarge'\n",
    "USE_BATCH_NORMALIZATION = False\n",
    "USE_CLASS_WEIGHTS = False\n",
    "BALANCE_TRAINING_CLASSES = False # Deprecated\n",
    "USE_PREPROCESSING = False\n",
    "BACKGROUND_THRESHOLD = 0.5 # if the background class has a probability higher than this threshold, the image is considered as background (set 1 if you want to disable this feature)\n",
    "SEED = 72121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19892,
     "status": "ok",
     "timestamp": 1731606779627,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "jOma3aMZ3r78",
    "outputId": "22ff0f8e-928a-4d7b-9589-38f489ad31b7"
   },
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/gdrive')\n",
    "    %cd /gdrive/My Drive/ANN/CHAL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10106,
     "status": "ok",
     "timestamp": 1731606789727,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "t9WZMOrc3r78"
   },
   "outputs": [],
   "source": [
    "from libraries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5049,
     "status": "ok",
     "timestamp": 1731606794770,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "qkuXQWtp3r79"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('data/training_set.npz', allow_pickle=True)\n",
    "\n",
    "# Divide data\n",
    "labels = data['labels']\n",
    "images = data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17975,
     "status": "ok",
     "timestamp": 1731606812740,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "eZbqvUNR3r7_",
    "outputId": "0078a656-f100-4fb3-d475-62ec30e389fc"
   },
   "outputs": [],
   "source": [
    "images, labels = clean_dataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1731606813021,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "KBXZkzP13r8A",
    "outputId": "daecb0e6-10b9-4be1-a10b-ffefa37be90b"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_and_balance_distribution(\n",
    "    images, labels, val_size=VAL_SPLIT, test_size=TEST_SPLIT, seed=SEED, balance_sets=BALANCE_SET, TEST=USE_TEST\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731606813022,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "0n7EfQH93r8B"
   },
   "outputs": [],
   "source": [
    "if USE_TEST:\n",
    "    # One-hot encode labels\n",
    "    y_train, y_val, y_test = one_hot_encode_labels(y_train, y_val, y_test)\n",
    "else:\n",
    "    y_train, y_val = one_hot_encode_labels(y_train, y_val, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3278,
     "status": "ok",
     "timestamp": 1731606816290,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "ipzti3eS3r8B",
    "outputId": "2861acc2-7107-4750-f709-8739f1d435b6"
   },
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "X_train, y_train = apply_mixup(X_train, y_train, alpha=ALPHA_MIXUP, factor=MIXUP_AUGMENT_FACTOR)\n",
    "print(\"X_train shape after mixup:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1731606816290,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "0g9AEOqV3r8C",
    "outputId": "f581f908-bb9f-4c49-db41-4e6b7ae1c8bc"
   },
   "outputs": [],
   "source": [
    "# Preprocess function suited for ConvNeXt models\n",
    "X_train = preprocess_input(X_train)\n",
    "X_test = preprocess_input(X_test)\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "output_shape = y_train[0].shape[0]\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Output shape: {output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731606816290,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "yYQkkqll3r8C"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    augmentation=None,\n",
    "    seed=SEED,\n",
    "    conv_layers=CONV_LAYERS,\n",
    "    dense_layers=DENSE_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    l2_regularization=L2_REGULARIZATION,\n",
    "    use_base_model=USE_BASE_MODEL,\n",
    "    background_threshold=BACKGROUND_THRESHOLD,\n",
    "    use_batch_normalization=USE_BATCH_NORMALIZATION,\n",
    "    nodes_per_layer=NODES_PER_LAYER,\n",
    "    use_preprocessing=USE_PREPROCESSING\n",
    "    ):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    relu_initialiser = tfk.initializers.HeNormal(seed=seed)\n",
    "    output_initialiser = tfk.initializers.GlorotNormal(seed=seed)\n",
    "    regularizer = tfk.regularizers.l2(l2_regularization)\n",
    "\n",
    "    # Define the input layer with original input shape\n",
    "    input_layer = tfk.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Preprocess the input image\n",
    "    if use_preprocessing:\n",
    "        x = PreprocessLayer(threshold=background_threshold)(input_layer)\n",
    "\n",
    "    else:\n",
    "        x = input_layer\n",
    "\n",
    "    if use_base_model:\n",
    "        # Load the VGG16 model with a custom input shape (96x96x3)\n",
    "        base_model = get_base_model(MODEL, input_shape=input_shape)\n",
    "\n",
    "        # Apply augmentation if specified\n",
    "        x = augmentation(x) if augmentation else x\n",
    "\n",
    "        x = base_model(x)\n",
    "        x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "    else:\n",
    "        # Apply augmentation if specified\n",
    "        x = augmentation(x) if augmentation else x\n",
    "\n",
    "        # Add Conv layers\n",
    "        x = tfkl.Conv2D(filters=16, kernel_size=3, activation='relu',\n",
    "                       padding='same', name='first_conv')(x)\n",
    "        x = tfkl.MaxPooling2D((2, 2), name='first_maxpool')(x)\n",
    "\n",
    "        for i in range(conv_layers - 1):\n",
    "            num_filters = 32 * (2 ** i)\n",
    "            x = tfkl.Conv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=3,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                name=f'conv_{num_filters}')(x)\n",
    "\n",
    "            if i < conv_layers - 2:  # Apply MaxPooling except for last conv layer\n",
    "                x = tfkl.MaxPooling2D((2, 2), name=f'maxpool_{num_filters}')(x)\n",
    "\n",
    "        # Apply GlobalAveragePooling2D after all conv layers\n",
    "        x = tfkl.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "\n",
    "    # Add Dense layers\n",
    "    for i in range(dense_layers):\n",
    "        x = tfkl.Dense(int(nodes_per_layer/(2**i)),\n",
    "                      activation='relu',\n",
    "                      name=f'dense_{i+1}',\n",
    "                      kernel_initializer=relu_initialiser)(x)\n",
    "\n",
    "        if use_batch_normalization:\n",
    "            x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            x = tfkl.Dropout(dropout_rate, name=f'dropout_{i+1}')(x)\n",
    "\n",
    "    output_layer = tfkl.Dense(output_shape,\n",
    "                             activation='softmax',\n",
    "                             name='output_layer',\n",
    "                             kernel_initializer=output_initialiser,\n",
    "                             kernel_regularizer=regularizer\n",
    "                             if l2_regularization > 0 else None)(x)\n",
    "\n",
    "    # Create model\n",
    "    model = tfk.Model(input_layer, output_layer)\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tfk.optimizers.Adam(learning_rate=learning_rate),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731606816291,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "1--_r-Yw3r8D"
   },
   "outputs": [],
   "source": [
    "if AUGMENTATION:\n",
    "    augmentation_layers = tfk.Sequential([\n",
    "        #tfkl.RandomFlip('horizontal'),\n",
    "        #tfkl.RandomFlip('vertical'),\n",
    "        #tfkl.RandomRotation(0.3),\n",
    "        #tfkl.RandomTranslation(0.4, 0.4, fill_mode='nearest'),\n",
    "        tfkl.RandomCrop(64, 64),\n",
    "        tfkl.RandomZoom(0.3, fill_mode='nearest'),\n",
    "        tfkl.Resizing(96, 96)\n",
    "    ], name='augmentation')\n",
    "\n",
    "    augmentation = ConditionalAugmentation(augmentation_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4679,
     "status": "ok",
     "timestamp": 1731606820966,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "juGlBDUZ3r8D",
    "outputId": "b8f5faf9-ba21-4d24-9b15-5f530fcb6793"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    augmentation=augmentation if AUGMENTATION else None\n",
    ")\n",
    "\n",
    "if SHOW_MODEL_SUMMARY:\n",
    "    model.summary()\n",
    "\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "checkpoint_callback = tfk.callbacks.ModelCheckpoint(\n",
    "    'models/best_model_restored.keras',  # Path where the model will be saved\n",
    "    monitor='val_accuracy',  # Metric to monitor\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    verbose=1,  # Print messages when saving the model\n",
    "    save_weights_only=False,  # Save the entire model (including architecture)\n",
    "    mode='max'  # 'max' to save the model with the highest validation accuracy\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, checkpoint_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weights(y_train) if USE_CLASS_WEIGHTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1859622,
     "status": "error",
     "timestamp": 1731608680585,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "rmM8aE0p3r8D",
    "outputId": "492123b0-b9c8-4bc3-ce81-dbf8e96c21b8"
   },
   "outputs": [],
   "source": [
    "# Train the model with early stopping callback\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ").history\n",
    "\n",
    "final_val_acc = history['val_accuracy'][-(PATIENCE+1)] * 100\n",
    "print(f'Final validation accuracy: {final_val_acc:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1731608680587,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "qN6L129O3r8E"
   },
   "outputs": [],
   "source": [
    "# Create a timestamp for the filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# save model using val acc\n",
    "model.save(f'models/model_{final_val_acc:.0f}_{timestamp}.keras')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1731608680588,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "He3liZWo3r8E"
   },
   "outputs": [],
   "source": [
    "# Function to log model parameters to a text file\n",
    "def log_model_parameters(final_val_acc, timestamp):\n",
    "    # Create the log filename with date and time\n",
    "    log_filename = f'models/model_{final_val_acc:.0f}_params_{timestamp}.txt'\n",
    "\n",
    "    # Write the parameters to the log file\n",
    "    with open(log_filename, 'w') as log_file:\n",
    "        log_file.write(\"Model Training Parameters:\\n\\n\")\n",
    "        log_file.write(f\"BATCH_SIZE: {BATCH_SIZE}\\n\")\n",
    "        log_file.write(f\"EPOCHS: {EPOCHS}\\n\")\n",
    "        log_file.write(f\"LEARNING_RATE: {LEARNING_RATE}\\n\")\n",
    "        log_file.write(f\"AUGMENTATION: {AUGMENTATION}\\n\")\n",
    "        log_file.write(f\"MIXUP: {MIXUP}\\n\")\n",
    "        log_file.write(f\"CONV_LAYERS: {CONV_LAYERS}\\n\")\n",
    "        log_file.write(f\"DENSE_LAYERS: {DENSE_LAYERS}\\n\")\n",
    "        log_file.write(f\"NODES_PER_LAYER: {NODES_PER_LAYER}\\n\")\n",
    "        log_file.write(f\"DROPOUT_RATE: {DROPOUT_RATE}\\n\")\n",
    "        log_file.write(f\"PATIENCE: {PATIENCE}\\n\")\n",
    "        log_file.write(f\"L2_REGULARIZATION: {L2_REGULARIZATION}\\n\")\n",
    "        log_file.write(f\"USE_BASE_MODEL: {USE_BASE_MODEL}\\n\")\n",
    "        log_file.write(f\"USE_BATCH_NORMALIZATION: {USE_BATCH_NORMALIZATION}\\n\")\n",
    "        log_file.write(f\"USE_CLASS_WEIGHTS: {USE_CLASS_WEIGHTS}\\n\")\n",
    "        log_file.write(f\"BALANCE_TRAINING_CLASSES: {BALANCE_TRAINING_CLASSES}\\n\")\n",
    "        log_file.write(f\"SEED: {SEED}\\n\")\n",
    "\n",
    "\n",
    "# Log the model parameters\n",
    "log_model_parameters(final_val_acc, timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1731608680588,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "V2mkb9rD3r8E"
   },
   "outputs": [],
   "source": [
    "# plot training loss and accuracy\n",
    "def plot_training(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axs[0].plot(history['loss'], label='train')\n",
    "    axs[0].plot(history['val_loss'], label='validation')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history['accuracy'], label='train')\n",
    "    axs[1].plot(history['val_accuracy'], label='validation')\n",
    "    axs[1].set_title('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zklVKxkk3r8E"
   },
   "source": [
    "# Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1731608680588,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "VKzKsUVL3r8E"
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "#model = tfk.models.load_model(f'models/model_{final_val_acc:.0f}_{timestamp}.keras', custom_objects={'PreprocessLayer': PreprocessLayer, 'ConditionalAugmentation': ConditionalAugmentation})\n",
    "model = tfk.models.load_model(f'models/best_model_restored.keras', custom_objects={'PreprocessLayer': PreprocessLayer, 'ConditionalAugmentation': ConditionalAugmentation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1731608680589,
     "user": {
      "displayName": "Isabella Balauca",
      "userId": "10841323877148049504"
     },
     "user_tz": -60
    },
    "id": "S901W4VC3r8F"
   },
   "outputs": [],
   "source": [
    "# Main testing logic\n",
    "if USE_TEST_BIG:\n",
    "    test_data = np.load('data/blood_cells_96x96.npz', allow_pickle=True)\n",
    "    test_images = test_data['data']\n",
    "    test_labels = test_data['labels']\n",
    "    \n",
    "\n",
    "    # Divide in 10 groups of tot images\n",
    "    N = 500\n",
    "    M = 10\n",
    "    images_per_class = N // 8\n",
    "    \n",
    "    test_accuracy = 0.0\n",
    "    test_precision = 0.0\n",
    "    test_recall = 0.0\n",
    "    test_f1 = 0.0\n",
    "    for k in range(M):\n",
    "        \n",
    "        print(f'Group {k+1}/{M}')\n",
    "        \n",
    "        # choose images_per_class images for each class\n",
    "        group_labels = []\n",
    "        group_indices = []\n",
    "        group_images = []\n",
    "    \n",
    "        for i in range(8):\n",
    "            indexes = np.where(test_labels == i)[0]\n",
    "            np.random.shuffle(indexes)\n",
    "            group_indices.extend(indexes[:images_per_class])\n",
    "            \n",
    "        group_labels = test_labels[group_indices]\n",
    "        group_images = test_images[group_indices]\n",
    "        \n",
    "        # Predict class probabilities and get predicted classes for normal test set\n",
    "        test_predictions = model.predict(group_images, verbose=1)\n",
    "        test_predictions_classes = np.argmax(test_predictions, axis=-1)\n",
    "        \n",
    "        # Calculate and display metrics for the normal test set\n",
    "        test_accuracy += accuracy_score(group_labels, test_predictions_classes)\n",
    "        test_precision += precision_score(group_labels, test_predictions_classes, average='weighted')\n",
    "        test_recall += recall_score(group_labels, test_predictions_classes, average='weighted')\n",
    "        test_f1 += f1_score(group_labels, test_predictions_classes, average='weighted')\n",
    "\n",
    "    test_accuracy /= M\n",
    "    test_precision /= M\n",
    "    test_recall /= M\n",
    "    test_f1 /= M\n",
    "    \n",
    "    print(f'Accuracy score over the normal test set: {round(test_accuracy, 4)}')\n",
    "    print(f'Precision score over the normal test set: {round(test_precision, 4)}')\n",
    "    print(f'Recall score over the normal test set: {round(test_recall, 4)}')\n",
    "    print(f'F1 score over the normal test set: {round(test_f1, 4)}')\n",
    "    \n",
    "    \n",
    "elif USE_TEST:\n",
    "    test_data = np.load('data/test_set.npz', allow_pickle=True)\n",
    "    test_images = test_data['data']\n",
    "    test_labels = test_data['labels']\n",
    "    \n",
    "    # Predict class probabilities and get predicted classes for normal test set\n",
    "    test_predictions = model.predict(test_images, verbose=0)\n",
    "    test_predictions_classes = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "    # Extract ground truth classes\n",
    "    test_gt = np.argmax(test_labels, axis=-1)\n",
    "\n",
    "    # Calculate and display metrics for the normal test set\n",
    "    test_accuracy = accuracy_score(test_gt, test_predictions_classes)\n",
    "    test_precision = precision_score(test_gt, test_predictions_classes, average='weighted')\n",
    "    test_recall = recall_score(test_gt, test_predictions_classes, average='weighted')\n",
    "    test_f1 = f1_score(test_gt, test_predictions_classes, average='weighted')\n",
    "\n",
    "    print(f'Accuracy score over the normal test set: {round(test_accuracy, 4)}')\n",
    "    print(f'Precision score over the normal test set: {round(test_precision, 4)}')\n",
    "    print(f'Recall score over the normal test set: {round(test_recall, 4)}')\n",
    "    print(f'F1 score over the normal test set: {round(test_f1, 4)}')\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(test_gt, test_predictions_classes)\n",
    "\n",
    "    # Plot the confusion matrix with class labels\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d',\n",
    "                xticklabels=[f'Class {i}' for i in range(8)],\n",
    "                yticklabels=[f'Class {i}' for i in range(8)], cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report for detailed metrics per class\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(test_gt, test_predictions_classes))\n",
    "\n",
    "    # ROC-AUC score for each class (only if this is multilabel or multiclass with probability predictions)\n",
    "    y_test_binarized = label_binarize(test_gt, classes=range(8))\n",
    "    roc_auc_scores = []\n",
    "    for i in range(8):\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test_binarized[:, i], test_predictions[:, i])\n",
    "            roc_auc_scores.append(roc_auc)\n",
    "            print(f\"Class {i} ROC-AUC Score: {round(roc_auc, 4)}\")\n",
    "        except ValueError:\n",
    "            print(f\"Class {i} ROC-AUC Score: Unable to calculate (not enough samples).\")\n",
    "\n",
    "    # Optional: Display mean ROC-AUC score across classes\n",
    "    if roc_auc_scores:\n",
    "        print(f\"\\nMean ROC-AUC Score: {round(np.mean(roc_auc_scores), 4)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "annEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
