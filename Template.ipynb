{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Artificial Neural Networks and Deep Learning 2024\n","---\n","\n","## Homework 1: Blood-cell classification\n","\n","### Team Members\n","- Maria Aurora Bertasini*\n","- Marco Cioci*\n","- Francesco Rosnati*\n","- Luca Tramacere*\n","\n","*Master's candidate in High Performance Computing Engineering at Politecnico di Milano"]},{"cell_type":"markdown","metadata":{},"source":["![classes](./img/classes.png)"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","---\n","\n","### Template Notebook\n","This notebook serves as a foundational template that outlines the workflow followed throughout the project. It provides an overview of the core steps involved in our process and includes links to more detailed, dedicated notebooks that focus on the key aspects of the project. Please note that **this template is not intended to be executed directly**. Instead, it serves as a reference to guide the reader through the structure and approach used. \n","\n","### Final model\n","[Final Model Notebook](./finalModel.ipynb) presents the final version of our model, that resulted in a 0.94 accuracy on Codabench Leaderbord.\n","\n","The model consist on a Transfer Learning approach that uses `EfficientNetB2` as backbone feature extractor. The classificator is composed of three medium-light dense layers (512, 256, 128), with proper batch normalizations and regularizations. \n","\n","The good results depend mainly on two focal aspects:\n","- heavy application of **augmentations**, especially from KerasCV library, to allow our models to generalize well beyond the limit imposed by the starting dataset\n","- a thorough journey in the choice of the best **fine-tuning** so that it could learn the best weights possible for the problem at hand\n","\n","### Report\n","The whole description of such journey is described in the attached [Report](./Report_homework1_YNWA.pdf)"]},{"cell_type":"markdown","metadata":{},"source":["## Work environment\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["### Manage Colab Environment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["COLAB = False\n","\n","if COLAB:\n","    !pip install keras_cv -qq\n","    from google.colab import drive\n","    drive.mount('/gdrive')\n","    %cd /gdrive/My Drive/ANN_new"]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15500,"status":"ok","timestamp":1732033480767,"user":{"displayName":"Rosario Napoli","userId":"16200523913878401815"},"user_tz":-60},"id":"hLHX8YwGCgaI","outputId":"a596fe05-339f-4d9a-a81a-afca468b5bbb"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import keras_cv as kcv\n","from tensorflow.keras.applications import \"\"\"BASE MODEL\"\"\"\n","from tensorflow.keras.applications.\"\"\"BASE MODEL\"\"\" import preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D, Dropout, BatchNormalization, Resizing, Rescaling, LeakyReLU, ELU\n","from sklearn.model_selection import train_test_splitBuild, Create, Compile\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","\n","SEED = 42"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing\n","\n","[Preprocessing Notebook](./datasetPreparation.ipynb): thorough description of the steps applied are shown in a dedicated environment, available at the link. \n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["Data required preprocessing before the actual ideation of the Neural Network could begin. In particular the dataset had a large number of *unwanted data*.\n","- Starting data, input of the preprocessing notebook: `data/training_set.npz`\n","- Processed data, output: `data/training_set_clean.npz`\n","\n","In the present notebook data are uploaded **already preprocessed** in the following cell."]},{"cell_type":"markdown","metadata":{},"source":["### Load the dataset already cleaned from unwanted data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = np.load('data/training_set_clean.npz')\n","X = data['images']\n","y = data['labels']"]},{"cell_type":"markdown","metadata":{},"source":["## Augmentations\n","\n","[Augmentations Notebook](./augmentations.ipynb) provides a detailed overview of all the *keras_cv* augmentations tested.\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["During the development of the project a large number of augmentations were experimented with. They come from two sources:\n","- *Keras Image augmentation layers* (included in Keras by default)\n","- *Keras_cv Augmentation Layers* (additional library)"]},{"cell_type":"markdown","metadata":{},"source":["#### Keras Image Augmentation Layers\n","Following empirical testing to determine which ones would consistently prove useful, a group of five layers was selected and established as the foundational augmentations. \n","From that point onward, all additional augmentations were applied on top of these base layers:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["augmentation = tf.keras.Sequential([\n","    tf.keras.layers.RandomFlip('horizontal'),\n","    tf.keras.layers.RandomRotation(0.7),\n","    tf.keras.layers.RandomBrightness(0.2),\n","    tf.keras.layers.RandomTranslation(height_factor=0.15, width_factor=0.15),\n","    tf.keras.layers.RandomZoom(0.3)\n","])"]},{"cell_type":"markdown","metadata":{},"source":["#### Keras_cv Augmentation Layers\n","For more complex and heavily distorting augmentations, the task was delegated to the more versatile and feature-rich *keras_cv* library, which provided greater flexibility and variety in data augmentation. Watch the dedicated notebook previously linked to find out more. It allows users to visualize the effects of each augmentation individually, examine the outcomes of various combinations, and review the pipelines actively employed throughout the project's development. \n","\n","\n","The augmentations are conveniently stored in a proper [python module](./py_modules/KerascvAug.py), once defined the desired ones they are to be inserted in the augment function below, which will later be applied during *Data Preparation* step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNm27z_40EmX"},"outputs":[],"source":["# import all augmentations and defined pipelines\n","from py_modules.kerascv_aug import *\n","\n","\n","# ------------------------- #\n","# Define pipeline/s\n","# ------------------------- #\n","# ...\n","# ...\n","   \n","\n","\n","def augment(images, labels):\n","    \n","    \n","    # Ensure images are tensors of the desired type\n","    images = tf.cast(images, tf.float32)\n","\n","\n","    # ------------------------- #\n","    # Apply augmentations here\n","    # ------------------------- #\n","    # ...\n","    # ...\n","    \n","\n","    return images, labels\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Transfer Learning\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["### Before Transfer Learning: custom CNNs\n","Initially, our approach was to build models from scratch, but it quickly became apparent that this strategy was much less efficient and powerful compared to leveraging pre-trained base models available in Keras. As a result, custom CNNs were soon abandoned in favor of more effective transfer learning methods. \n","\n","However, we have decided to present a couple of our early attempts for the sake of documentation and insight into the development process. These can be found in the [Custom CNNs notebook](./CustomCNNs.ipynb)."]},{"cell_type":"markdown","metadata":{},"source":["#### Prepare dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set autotune\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","# Normalize and preprocess images\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n","X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n","\n","# One-hot encode labels\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes=8).astype(np.float32)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=8).astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ---------------- #\n","# Preparation\n","# ---------------- #\n","\n","def prepare_dataset(images, labels, is_training=True, batch_size=32):\n","\n","    # Create the base dataset\n","    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n","\n","    if is_training:\n","        dataset = dataset.shuffle(buffer_size=1024)\n","\n","    # Apply EfficientNet preprocessing\n","    def preprocess(images, labels):\n","        images = preprocess_input(images)\n","        return images, labels\n","\n","    dataset = dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\n","\n","    # Batch before augmentation\n","    dataset = dataset.batch(batch_size)\n","\n","    if is_training:\n","\n","        # It is possbile to have different augments in different batches\n","        def augment_with_index(batch_index, data):\n","            images, labels = data\n","            return augment(images, labels, batch_index)\n","\n","        dataset = dataset.enumerate().map(\n","            augment_with_index, num_parallel_calls=AUTOTUNE\n","        )\n","\n","    return dataset.prefetch(buffer_size=AUTOTUNE)\n","\n","\n","# Prepare datasets\n","train_dataset = prepare_dataset(X_train, y_train, is_training=True, batch_size=32)\n","val_dataset = prepare_dataset(X_test, y_test, is_training=False, batch_size=32)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3588555,"status":"ok","timestamp":1732037136049,"user":{"displayName":"Rosario Napoli","userId":"16200523913878401815"},"user_tz":-60},"id":"k1XQ9nhW0fyv","outputId":"5b2e1c6a-2482-4730-9677-eb3c95fb7165"},"outputs":[],"source":["# -------------------- #\n","# Build, Create, Compile\n","# -------------------- #\n","\n","def create_model(input_shape=(96, 96, 3), num_classes=8, augmentation=None):\n","    input_layer = Input(shape=input_shape)\n","\n","    # Resizing layer for prediction to resize images to 224x224\n","    x = Resizing(260, 260)(input_layer)\n","\n","    # Base model\n","    base_model = \"\"\"BASE MODEL\"\"\"\"\n","    base_model.trainable = False\n","\n","    # Model architecture\n","    # with Activation Function LeakyReLU\n","    x = augmentation(x)\n","    # x = Rescaling(scale=1./127.5, offset=-1)(x)\n","    x = base_model(x, training=False)\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dropout(0.2)(x)\n","    x = BatchNormalization()(x)\n","    x = Dense(512, activation=None)(x)\n","    x = LeakyReLU(negative_slope=0.05)(x)\n","    x = Dropout(0.1)(x)\n","    x = BatchNormalization()(x)\n","    x = Dense(256, activation=None)(x)\n","    x = LeakyReLU(negative_slope=0.05)(x)\n","    x = Dropout(0.1)(x)\n","    x = BatchNormalization()(x)\n","    x = Dense(128, activation=None)(x)\n","    x = LeakyReLU(negative_slope=0.05)(x)\n","    output_layer = Dense(num_classes, activation='softmax')(x)\n","\n","    return Model(inputs=input_layer, outputs=output_layer)\n","\n","\n","# ---------------- #\n","# Create & Compile\n","# ---------------- #\n","\n","model = create_model(augmentation=augmentation)\n","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.001,\n","    decay_steps=1000,\n","    decay_rate=0.95\n",")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","model.compile(\n","    optimizer=optimizer,\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","\n","# ---------------- #\n","# Other settings\n","# ---------------- #\n","\n","# Callbacks\n","callbacks = [\n","    EarlyStopping(\n","        monitor='val_accuracy',\n","        patience=15,\n","        restore_best_weights=True\n","    ),\n","    ModelCheckpoint(\n","        \"\"\"MODEL NAME\"\"\",\n","        monitor='val_accuracy',\n","        save_best_only=True,\n","        mode='max'\n","    )\n","]\n","\n","# Compute class weights to fight class imbalance\n","class_weights = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(np.argmax(y_train, axis=1)),\n","    y=np.argmax(y_train, axis=1)\n",")\n","class_weights = dict(enumerate(class_weights))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ---------------- #\n","# Train model\n","# ---------------- #\n","\n","history = model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=100,\n","    callbacks=callbacks,\n","    class_weight=class_weights\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Fine-tuning\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize the Base Model architecture\n","Every Keras pre-trained model has a different architecture, some of them have layers structured in blocks of layers and stages of blocks. It is advisable to unfreeze whole blocks, in order to do so it is necessary to visualize the feature extractor layers."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print layer indices, names, and trainability status\n","for i, layer in enumerate(model.get_layer(\"\"\"BASE MODEL\"\"\").layers):\n","    print(f\"Layer {i}: {layer.name}, Type: {type(layer).__name__}, Trainable: {layer.trainable}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tune the desired amount of blocks\n","Once established the number of layers to keep freezed, it's possible to set such number with parameter `N` and procede with the fine tuning, as shown in the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1031511,"status":"ok","timestamp":1732038167546,"user":{"displayName":"Rosario Napoli","userId":"16200523913878401815"},"user_tz":-60},"id":"YzfoN6Q8zCum","outputId":"292abbc5-0ca3-4c0e-fb3b-effeaee77efe"},"outputs":[],"source":["# Reload model\n","model = tf.keras.models.load_model(\"\"\"MODEL NAME\"\"\")\n","\n","\n","# ---------------- #\n","# Unfreeze\n","# ---------------- #\n","\n","N = \"\"\"CHOOSE NUMBER\"\"\" # Number of layers to freeze\n","\n","for i, layer in enumerate(model.get_layer(\"\"\"BASE MODEL\"\"\").layers):\n","    layer.trainable = True\n","\n","for i, layer in enumerate(model.get_layer(\"\"\"BASE MODEL\"\"\").layers):\n","    layer.trainable = False\n","\n","\n","for i, layer in enumerate(model.get_layer(\"\"\"BASE MODEL\"\"\").layers):\n","    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n","        layer.trainable = True\n","\n","\n","# Set the first N layers as non-trainable\n","for i, layer in enumerate(model.get_layer(\"\"\"BASE MODEL\"\"\").layers[:N]):\n","    layer.trainable = False\n","\n","# Print layer indices, names, and trainability status\n","for i, layer in enumerate(model.get_layer(\"\"\"BASE MODEL\"\"\").layers):\n","    print(f\"Layer {i}: {layer.name}, Type: {type(layer).__name__}, Trainable: {layer.trainable}\")\n","\n","\n","\n","# -------------------- #\n","# Fine-tune settings\n","# -------------------- #\n","\n","# Use a lower learning rate for fine-tuning\n","fine_tune_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.00001,  # Small learning rate for fine-tuning\n","    decay_steps=1000,\n","    decay_rate=0.95\n",")\n","fine_tune_optimizer = tf.keras.optimizers.Adam(learning_rate=fine_tune_lr_schedule)\n","model.compile(optimizer=fine_tune_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Additional callbacks\n","fine_tune_early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n","fine_tune_checkpoint = ModelCheckpoint(\"\"\"MODEL NAME FINE TUNED\"\"\", monitor='val_accuracy', save_best_only=True, mode='max')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# -------------------- #\n","# Fine-tune\n","# -------------------- #\n","\n","fine_tune_history = model.fit(\n","    train_dataset,\n","    batch_size=16, # Smaller batch size for fine-tuning\n","    validation_data=val_dataset,\n","    epochs=20,\n","    callbacks=[fine_tune_early_stopping, fine_tune_checkpoint],\n","    class_weight=class_weights\n",").history"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":".kerasVenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"}},"nbformat":4,"nbformat_minor":0}
